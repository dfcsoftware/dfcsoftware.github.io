[     {
     "title": "Welcome back my Friends",
     "url": "https://dfcsoftware.github.io/my-super-post.html",
     "body": "Linux-in-the-House.svg Introduction Setup a New Server Dedicated to my wife, Linh. She fixes the meals, I fix the house. Following is a review of things to think about. Early one foggy morning a white van whipped into the driveway and out pops two bearded fellows. \"Hey, the wallpaper removers are here\" I called out. Sure enough the camera alerted me as to an abnormal presence in the force. It\u0027s not always been so easy, and assembling a security camera setup was something from Mad magazine in the 1960\u0027s that I achieved to some degree only lately. With an un-finished basement it was a breeze to run Cat-6 wires to each camera, even supplying power over ethernet (POE) to a latest breed of observers. Following our birth to computers, from mechanical to electrical, monolithic to personal, microscopic chemicals and minerals beat their little binary hearts digested from human text. Standing on the hills of our ancestors we can only hope to provide something the world wants to use as long it works. Thinking of such lofty goals, I am inspired to continue my journaley writings. From here, for now; -- Don Lock your computer\u0027s doors and windows, but remember: A house is not a home without visitors"
     }
,     {
     "title": "Welcome to the Forest",
     "url": "https://dfcsoftware.github.io/welcome-to-the-forest.html",
     "body": "Linux in House Dedicated to my wife, Linh. She fixes the meals, I fix the house. Connect all your trees into the forest. The Forest"
     }
,     {
     "title": "Alerts",
     "url": "https://dfcsoftware.github.io/alerts.html",
     "body": "Finally got my alerts working, so when a host goes haywire I get an alert on the Phone and Cloud, with E-Mail as backup. Package is here: https://linux-in-the-house.pages.dev/2023/august#download #linux #alert #opensource"
     }
,     {
     "title": "QEMU Virtual Machine",
     "url": "https://dfcsoftware.github.io/qemu-virtual-machine.html",
     "body": "Virtual machines using #qemu is breezy/easy with #Cockpit on #AlmaLinux. So glad I blasted an old machine to start my new resiliency strategy without proprietary VirtualBox or VMWare. Check out some notes here: \ufeffhttps://linux-in-the-house.pages.dev/2023/june #linux #opensource #vm"
     }
,     {
     "title": "Long Ethernet",
     "url": "https://dfcsoftware.github.io/long-ethernet.html",
     "body": "Anyone ever extend an Ethernet connection over 1000 feet (350 meters)? Did you use coax? I need to wire up another building with my existing router. No line of sight, and probably under ground (sand). #networking #router #ethernet #coax #linux Thanks \ud83d\ude00"
     }
,     {
     "title": "Networks",
     "url": "https://dfcsoftware.github.io/networks.html",
     "body": "The command line offers a wealth of network management commands. Here are 10 of my favorites. https://linux-in-the-house.pages.dev/2023/april #linux #opensource #networking PS: Not sure I like my new avatar Wall-E, missing the penguin..."
     }
,     {
     "title": "Encrypt Files",
     "url": "https://dfcsoftware.github.io/encrypt-files.html",
     "body": "Protect your carefully built system and it\u0027s files from abuse, tampering, or just plain spying. It\u0027s easy to do, what are you waiting for? Encrypt your data directory now. Also #thunderbird and #evolution support signing and encryption of E-Mails. In the March edition: https://linux-in-the-house.pages.dev/2023/march #linux #opensource #encryption #openpgp"
     }
,     {
     "title": "VLAN",
     "url": "https://dfcsoftware.github.io/vlan.html",
     "body": "I normally run NFS over a VLAN, and Virtual IP stacking is very interesting, so I documented it. Turns out it\u0027s a good way to set up a guest LAN too if your router supports it and managed switches help a lot. https://linux-in-the-house.pages.dev/2023/february #linux #opensource #vlan"
     }
,     {
     "title": "Docker",
     "url": "https://dfcsoftware.github.io/docker.html",
     "body": "Migrating my blog, first step is some useful common commands to check out the status of docker containers. Glad that **docker prune** saved me over 2GB disk space! https://linux-in-the-house.pages.dev/2023/january #linux #opensource #docker"
     }
,     {
     "title": "Media Management",
     "url": "https://dfcsoftware.github.io/media-management.html",
     "body": "Hey I was wondering what people use for Media management, like TV, Movies and Music? I finally settled on HDHomeRun, Plex, MPD, and curseradio. Here are some notes on using them: https://linux-in-the-house.pages.dev/Media_Managment #linux #opensource #plex #hdhomerun #mpd #curseradio"
     }
,     {
     "title": "BeagleBone",
     "url": "https://dfcsoftware.github.io/beaglebone.html",
     "body": "The next best project to #homeassistant is running it on a #beaglebone SBC. Quite, low power and small. Excited to see #BeaglePlay released! So here\u0027s my notes: https://linux-in-the-house.pages.dev/BeagleBone #linux #opensource"
     }
,     {
     "title": "MiniFlux",
     "url": "https://dfcsoftware.github.io/miniflux.html",
     "body": "If you like to read RSS on several devices, host your own reader, #miniflux Read on one device and pick back up on another without having to skip already read posts. No app required on your device. A small footprint, written in go. https://linux-in-the-house.pages.dev/Miniflux #linux #opensource"
     }
,     {
     "title": "Matrix",
     "url": "https://dfcsoftware.github.io/matrix.html",
     "body": "Unable to shake the #sysadmin in me, my solution to alerting left the beeper ;-) behind and now a watch zaps my arm using #matrix on a home server. \ud83c\udf29\ufe0f It\u0027s a leading contender for #nextcloud talk on the phone, since it\u0027s faster, but seems more difficult to set up and maintain ( Hint: #curl ) Hoping matter bridge helps in the future. Here\u0027s some notes on setting up Matrix-Synapse on a #homeserver https://linux-in-the-house.pages.dev/Matrix #linux #opensource"
     }
,     {
     "title": "HomeAutomation",
     "url": "https://dfcsoftware.github.io/homeautomation.html",
     "body": "Finally finished the chapter on #homeautomation with #homeassistant. One of my favorite projects at home. Didn\u2019t get a too crazy and docker simplifies a lot. Wall-E is the favorite https://linux-in-the-house.pages.dev/HomeAssistant #linux #opensource"
     }
,     {
     "title": "EMail Soup",
     "url": "https://dfcsoftware.github.io/email-soup.html",
     "body": "Just completed a deep dive into E-Mail alphabet soup, PTR DNS, SPF, DKIM, DMARC, whew! Actually self-hosting E-Mail is easier once you understand how they work. I appreciate the pipe loops in Postfix, Unix sockets, and all the RFCs in between. https://linux-in-the-house.pages.dev/E-Mail_Relay #linux #opensource #email"
     }
,     {
     "title": "Cloud",
     "url": "https://dfcsoftware.github.io/cloud.html",
     "body": "A cloud used to be floating in the sky, then it was a bubbly thing on a network diagram where weird things happen. Now a cloud just means \"Somebody elses computer.\". My notes on setting up your *Self-Hosted* cloud: https://linux-in-the-house.pages.dev/Cloud #linux #opensource #cloud"
     }
,     {
     "title": "PostgreSQL",
     "url": "https://dfcsoftware.github.io/postgresql.html",
     "body": "My favorite database, #postgresql has captured current attention after years of using #oracle on #redhat . Love reading Cooper Press @cooperpress https://postgresweekly.com/ and using PL/pgsql https://www.postgresql.org/docs/current/plpgsql.html. As usual, here are my notes on PostgreSQL: https://linux-in-the-house.pages.dev/Postgresql #linux #opensource #db"
     }
,     {
     "title": "EMail",
     "url": "https://dfcsoftware.github.io/email.html",
     "body": "Running an e-mail server at home required many e-mails strung together to make it work. I put them in a chapter of the book \u201cLinux in the House\u201d The oldest internet app\u2026 https://linux-in-the-house.pages.dev/E-Mail #linux #email #opensource Hope this helps,\u2014Don"
     }
,     {
     "title": "Setup Server",
     "url": "https://dfcsoftware.github.io/setup-server.html",
     "body": "Set Up a New Server I use this as a guide to enable proper monitoring and maintenance of any new server on the network. Table of Contents Set Up a New Server Sample Home Network: Check your Network Update Package Repository Debian RedHat New User and Group to Use Debian RedHat Also set the root password Firewall Debian RedHat Block bad actors, whole Autonomous System [1] groups at a time Set your host and domain names Rsyslog - Send Syslog Entries to Remote Syslog Host Local System Remote Syslog Host Restart rsyslog Time Control timezone RedHat Debian E-Mail - Client for Sending Local Mail Identify Mail Client Host and Domain Mail Transport Agent (MTA) packages MAC OS send mail to local server, not system configured in mail app Test mail Monit - Monitor System and Restart Processes Installation Configuration Process Munin - Resource History Monitor Alert via e-mail: Adjust disk full thresholds: Graph Example Redhat Alternative Fail2ban - Automatic Firewall Blocking Install Configure Backup - Save Your Files Daily Automatic backup to USB disk Automatic Backup of PostgreSQL Database cron.daily Rsync - Remote File Synchronization Schedule Script Logwatch - Daily Alert of Logging Activity Installation Schedule Add services Logcheck - mails anomalies in the system logfiles to the admin Installation Schedule Change email destination Sysstat - Gather System Usage Statistics Installation Configuration Past Statistics Report Sample Runs S.M.A.R.T. Disk Monitoring Install software: Configure Restart Monitoring script for testing and reporting. smartd database Login Notices to Users - motd/issue/issue.net Installation Configuration Verify Example login Login notification Continue Sample Home Network: Sometimes the Modem, Router and Main Switch are one unit, or there is no modem. -----\u003e Single Arrow is a limited access network (VLAN) \u003c----\u003e Double Arrows is an Open Network The key point here is that the servers are isolated on a separate switch for performance and security reasons, using a VLAN (Virtual Local Area Network) local to the Server Switch. Server VLAN network packets between each other never leave the Server Switch. Each server has another IP address not on the VLAN for public access. A guest WiFi service does not have access to the Main Switch because it is on it\u0027s own VLAN, so local resources are protected from that experimental 12 year old guest. If my camera accesses a cloud service (most do), then I link it to the Guest WiFi for security purposes. Any other untrusted device will also be on the Guest WiFi, like Robot Vacuum Cleaners, Car Chargers, Car, TV streaming box, VOIP (Telephone VoiceOverIP), Garage Door Opener, Door Locks, etc... Reference: https://www.graphviz.org/doc/info/colors.html Graph also available in: bgcolor=\"transparent\"; Check your Network See Network Interfaces The Debian Way $ ifconfig eno1: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 192.168.0.8 netmask 255.255.255.0 broadcast 192.168.0.255 inet6 fe80::0000:1111:3333:2222 prefixlen 64 scopeid 0x20\u003clink\u003e ether 2a:53:9b:00:f9:21 txqueuelen 1000 (Ethernet) RX packets 342047883 bytes 378663045018 (378.6 GB) RX errors 0 dropped 54131 overruns 0 frame 0 TX packets 221663343 bytes 165067861773 (165.0 GB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 device interrupt 16 memory 0xc0b00000-c0b20000 lo: flags=73\u003cUP,LOOPBACK,RUNNING\u003e mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u003chost\u003e loop txqueuelen 1000 (Local Loopback) RX packets 32619960 bytes 99080107335 (99.0 GB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 32619960 bytes 99080107335 (99.0 GB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 The RedHat and Newer Debian Way # ip a 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens3: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 36:27:10:52:32:96 brd ff:ff:ff:ff:ff:ff altname enp0s3 inet 192.168.0.5/24 brd 10.0.2.255 scope global dynamic noprefixroute ens3 valid_lft 66645sec preferred_lft 66645sec inet6 fec0::000:ff:1111:1111/64 scope site dynamic noprefixroute valid_lft 86390sec preferred_lft 14390sec inet6 fe80::000:ff:1111:1111/64 scope link noprefixroute valid_lft forever preferred_lft forever System Log is The Debian Way /var/log/syslog The RedHat Way /var/log/messages Bring Network Up: The Debian Way $ sudo cat /etc/netplan/01-network-manager-all.yaml network: version: 2 renderer: networkd ethernets: enp3s0: dhcp4: true $ sudo netplan try $ sudo netplan -d apply $ sudo systemctl restart system-networkd Reference: https://netplan.io/ The RedHat Way $ sudo nmcli connection up ens3 vi /etc/sysconfig/network-scripts/ifcfg-ens3 ~ ONBOOT=yes ~ :wq Reference: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_networking/configuring-an-ethernet-connection_configuring-and-managing-networking Update Package Repository Debian Always refresh the package repository before getting started. $ sudo apt-get update You may need to add extra repositories, just check the sources. The four main repositories are: Main - Canonical-supported free and open-source software. Universe - Community-maintained free and open-source software. Restricted - Proprietary drivers for devices. Multiverse - Software restricted by copyright or legal issues. Backports[1] - Software from older releases that should still work. Reference: Backports for old stuff you really really need. Universe is a typical add-on, beware of unstable. $ sudo add-apt-repository universe $ sudo grep \u0027^deb \u0027 /etc/apt/sources.list ... deb http://us.archive.ubuntu.com/ubuntu/ jammy universe ... $ sudo add-apt-repository --remove universe RedHat Always refresh the package repository before getting started. $ sudo dnf update Now I can disable subscription-manager since I do not have a RedHat subscription. Change: From -\u003e enabled=1 To -\u003e enabled=0 In file: /etc/yum/pluginconf.d/subscription-manager.conf $ sudo vi /etc/yum/pluginconf.d/subscription-manager.conf $ sudo yum clean all 0 files removed You may need to add extra repositories, just check the sources. The EPEL repository provides additional high-quality packages for RHEL-based distributions. EPEL is a selection of packages from Fedora, but only packages that are not in RHEL or its layered products to avoid conflicts. The folks at Fedora have very nicely put up an automatic build and repo system and they are calling it COPR (Cool Other Package Repositories). $ sudo dnf install epel-release \u0027dnf-command(copr)\u0027 Reference: https://www.redhat.com/sysadmin/install-epel-linux https://copr.fedorainfracloud.org/ New User and Group to Use Be sure to match the same user numbers across systems, because when sharing files using NFS, the numbers need to match. $ sudo adduser don --uid 1001 Adding user `don\u0027 ... Adding new group `don\u0027 (1001) ... Adding new user `don\u0027 (1001) with group `don\u0027 ... Creating home directory `/home/don\u0027 ... Copying files from `/etc/skel\u0027 ... New password: Retype new password: passwd: password updated successfully Changing the user information for don Enter the new value, or press ENTER for the default . . . Full Name []: Don . . . Room Number []: . . . Work Phone []: . . . Home Phone []: . . . Other []: Is the information correct? [Y/n] y Adding new user `don\u0027 to extra groups ... Adding user `don\u0027 to group `dialout\u0027 ... Adding user `don\u0027 to group `i2c\u0027 ... Adding user `don\u0027 to group `spi\u0027 ... Adding user `don\u0027 to group `cdrom\u0027 ... Adding user `don\u0027 to group `floppy\u0027 ... Adding user `don\u0027 to group `audio\u0027 ... Adding user `don\u0027 to group `video\u0027 ... Adding user `don\u0027 to group `plugdev\u0027 ... Adding user `don\u0027 to group `users\u0027 ... If this user is an administrator; Debian $ sudo usermod -aG sudo rootbk Check user for group \u002727(sudo)\u0027. $ id rootbk uid=1002(rootbk) gid=1002(rootbk) groups=1002(rootbk),20(dialout),24(cdrom),25(floppy),27(sudo),29(audio),44(video),46(plugdev),100(users),114(i2c),993(spi) RedHat $ sudo usermod -aG wheel rootbk Check user for group \u002710(wheel)\u0027. # id don uid=1002(rootbk) gid=1002(rootbk) groups=1002(rootbk),10(wheel) Also set the root password in case the system will not boot $ sudo passwd root [sudo] password for don: New password: Retype new password: passwd: password updated successfully Firewall Every machine should have a firewall enabled, especially before connecting to the internet. Now adays there is a choice between Uncomplicated Firewall (ufw) and firewalld. I choose firewalld. Debian ufw - Uncomplicated Firewall $ sudo apt-get install ufw $ sudo ufw allow 22/tcp $ sudo ufw enable $ sudo ufw status numbered Status: active To Action From -- ------ ---- [ 1] 22/tcp ALLOW IN Anywhere [ 2] 22/tcp (v6) ALLOW IN Anywhere (v6) $ sudo ufw delete 2 $ sudo ufw logging low $ sudo ufw logging on NOTE: Logging values are: low|medium|high. Only network blocks are logged on low. Reference: https://launchpad.net/ufw RedHat firewalld $ sudo dnf install firewalld $ sudo firewall-cmd --add-service=ssh success $ sudo firewall-cmd --list-services cockpit dhcpv6-client ssh $ sudo firewall-cmd --remove-service=cockpit success $ sudo firewall-cmd --remove-service=dhcpv6-client success $ sudo firewall-cmd --list-services ssh $ sudo firewall-cmd --runtime-to-permanent success Reference: https://firewalld.org/ Block bad actors, whole Autonomous System [1] groups at a time Using IP addresses from Logwatch, Logcheck, and Logwatcher, feed them into the firewall.sh script. Run the git clone and install the dependencies based on your operating system. File: ~/firewall.sh #!/bin/bash ############################################################################## # # File: firewall.sh # # Purpose: Block IP address or CIDR range using OS firewall # # Dependencies: # # git clone https://github.com/nitefood/asn # # * For more detail get an ~/.asn/iqs_token # from https://github.com/nitefood/asn#ip-reputation-api-token # # * **Debian 10 / Ubuntu 20.04 (or newer):** # # ``` # apt -y install curl whois bind9-host mtr-tiny jq ipcalc grepcidr nmap ncat aha # ``` # * Enable ufw and allow the ports you need # # # ufw allow 22 # # ufw enable # # * Delete rules not needed: # # ufw status numbered # # ufw delete \u003cnumber\u003e # # * **CentOS / RHEL / Rocky Linux 8:** # # # Install repos: # $ sudo dnf repolist # repo id repo name # appstream CentOS Stream 9 - AppStream # baseos CentOS Stream 9 - BaseOS # epel Extra Packages for Enterprise Linux 9 - x86_64 # epel-next Extra Packages for Enterprise Linux 9 - Next - x86_64 # extras-common CentOS Stream 9 - Extras packages # # $ ls /etc/yum.repos.d # centos-addons.repo centos.repo epel-next.repo epel-next-testing.repo epel.repo epel-testing.repo redhat.repo # # dnf install bind-utils jq whois curl nmap ipcalc grepcidr aha # # If you have a list of IP addresses to block (text file, each IP on a separate line), # you can easily import that to your block list: # # # firewall-cmd --permanent --ipset=networkblock --add-entries-from-file=/path/to/blocklist.txt # # firewall-cmd --reload # # To view ipsets: # # firewall-cmd --permanent --get-ipsets # networkblock # # firewall-cmd --permanent --info-ipset=networkblock # networkblock # type: hash:net # options: maxelem=1000000 family=inet hashsize=4096 # entries: 46.148.40.0/24 # # # firewall-cmd --add-service=smtp # success # # firewall-cmd --add-service=smtps # success # # firewall-cmd --list-services # cockpit dhcpv6-client smtp smtps ssh # # firewall-cmd --remove-service=cockpit # success # # firewall-cmd --remove-service=dhcpv6-client # success # # firewall-cmd --list-services # smtp smtps ssh # # firewall-cmd --runtime-to-permanent # success # # # firewall-cmd --list-all # public (active) # target: default # icmp-block-inversion: no # interfaces: ens3 # sources: # services: smtp smtps ssh # ports: # protocols: # forward: no # masquerade: no # forward-ports: # source-ports: # icmp-blocks: # rich rules: # # https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/security_guide/sec-setting_and_controlling_ip_sets_using_firewalld # # * TO UNDO A MISTAKEN BLOCK: # # firewall-cmd --permanent --ipset=networkblock --remove-entry=x.x.x.x/y # # firewall-cmd --reload # # * To drop ipset # # firewall-cmd --permanent --delete-ipset=networkblock # # firewall-cmd --reload # # Author Date Description # ---------- -------- -------------------------------------------------------- # D. Cohoon Jan-2023 Created # D. Cohoon Feb-2023 Add RedHat firewalld ############################################################################## DIR=/root LOG=${DIR}/firewall.log WHO=/tmp/whois.txt CIDR=/tmp/whois.cidr IP=\"${1}\" OS=$(/usr/bin/hostnamectl|/usr/bin/grep \u0027Operating System\u0027|/usr/bin/cut -d: -f2|/usr/bin/awk \u0027{print $1}\u0027) #............................................................................. function set_ipset() { FOUND_IPSET=0 while read SET do if [ ! -z ${SET} ] \u0026\u0026 [ ${SET} == \"networkblock\" ]; then FOUND_IPSET=1 fi done \u003c\u003c\u003c $(sudo /usr/bin/firewall-cmd --permanent --get-ipsets) # if [ $FOUND_IPSET -eq 0 ]; then # Create networkblock ipset sudo /usr/bin/firewall-cmd --permanent --new-ipset=networkblock --type=hash:net \\ --option=maxelem=1000000 --option=family=inet --option=hashsize=4096 # Add new ipset to drop zone sudo /usr/bin/firewall-cmd --permanent --zone=drop --add-source=ipset:networkblock # reload sudo /usr/bin/firewall-cmd --reload fi } # #............................................................................. function run_asn() { ${DIR}/asn/asn -n ${IP} \u003e ${WHO} /usr/bin/cat ${WHO} RANGE=$(/usr/bin/cat ${WHO} | /usr/bin/grep \u0027NET\u0027 | /usr/bin/grep \u0027/\u0027 | /usr/bin/awk -Fm \u0027{print $6}\u0027 | /usr/bin/cut -d\" \" -f1) /usr/bin/echo \"CDR: ${RANGE}\" /usr/bin/echo \"${RANGE}\" \u003e ${CIDR} } #............................................................................. # if [ ${1} ]; then run_asn else /usr/bin/echo \"Usage: ${0} \u003cIP Address\u003e\" exit 1 fi #............................................................................. # /usr/bin/grep -v deaggregate ${CIDR} \u003e ${CIDR}.block while read -r IP do /usr/bin/echo \"$(/usr/bin/date) - OS: ${OS}\" | /usr/bin/tee -a ${LOG} /usr/bin/echo \"Blocking: ${IP}\" | /usr/bin/tee -a ${LOG} case ${OS} in AlmaLinux|CentOS) /usr/bin/echo \"Firewalld\" set_ipset sudo /usr/bin/firewall-cmd --permanent --ipset=networkblock --add-entry=${IP} sudo /usr/bin/firewall-cmd --reload ;; Ubuntu|Debian) /usr/bin/echo \"ufw\" sudo /usr/sbin/ufw prepend deny from ${IP} to any 2\u003e\u00261 |tee -a $LOG ;; esac done \u003c ${CIDR}.block https://www.arin.net/resources/guide/asn/ Set your host and domain names File: /etc/hosts 127.0.0.1 localhost 192.168.1.5 www.example.com example.com www # The following lines are desirable for IPv6 capable hosts ::1 localhost ip6-localhost ip6-loopback ff02::1 ip6-allnodes ff02::2 ip6-allrouters File: /etc/hostname don.example.com Rsyslog - Send Syslog Entries to Remote Syslog Host It is a good idea to send log messages to another host in case the system crashes. You will be able to see that last gasping breath of the dying server. Also in the event of a compromised system hackers usually zero out the local syslog to cover their tracks. Now you still have any trace of the hackers on the central rsyslog host. It makes things simpler for detailed log analysis with combined logs on one system too. Local System Replicate log entries: Add the following to cause log entries to be in /var/log/syslog locally and be sent to a remote syslog host. If you do not have a Remote Syslog Host, skip this. File: /etc/rsyslog.conf Add these lines on local system. ~ # Remote logging - Aug 2020 Don # Provides UDP forwarding *.* @192.168.1.5:514 #this is the logging host ~ Alert: Create the following to send syslog alerts to email if the severity is high (3 or below). File: /etc/rsyslog.d/alert.conf Create the file if it does not exist and replace with these lines. module(load=\"ommail\") template (name=\"mailBody\" type=\"string\" string=\"Alert for %hostname%:\\n\\nTimestamp: %timereported%\\nSeverity: %syslogseverity-text%\\nProgram: %programname%\\nMessage: %msg%\") template (name=\"mailSubject\" type=\"string\" string=\"[%hostname%] Syslog alert for %programname%\") if $syslogseverity \u003c= 3 and not ($msg contains \u0027brcmfmac\u0027) then { action(type=\"ommail\" server=\"192.168.1.3\" port=\"25\" mailfrom=\"rsyslog@localhost\" mailto=\"don@example.com\" subject.template=\"mailSubject\" template=\"mailBody\" action.execonlyonceeveryinterval=\"3600\") } Remote Syslog Host Allow remote hosts to log here: Open firewall port 514/udp on remote syslog host. $ sudo ufw allow 514/udp File: /etc/rsyslog.conf Add these lines to remote syslog host. ~ # provides UDP syslog reception module(load=\"imudp\") input(type=\"imudp\" port=\"514\") ~ # Process remote logs into seperate directories, then stop. Do not duplicate into syslog $template RemoteLogs,\"/var/log/%HOSTNAME%/%PROGRAMNAME%.log\" *.* ?RemoteLogs \u0026 stop Restart rsyslog $ sudo systemctl restart rsyslog Time Control All servers should be set up to synchronize their time over the network using Network Time Protocol (NTP). This is critical in validating security certificates. For offline systems, consider using a Real Time Clock (RTC) attached to something like BeagleBone. TODO: Link to Beaglebone timezone Change to match your timezone. File: /etc/timezone $ cat /etc/timezone America/New_York Set timezone with timedatactl, and verify. $ sudo timedatectl set-timezone America/New_York $ timedatectl Local time: Sun 2022-10-09 18:27:11 EDT Universal time: Sun 2022-10-09 22:27:11 UTC RTC time: n/a Time zone: America/New_York (EDT, -0400) System clock synchronized: yes NTP service: active RTC in local TZ: no RedHat RedHat uses chronyd service File: /etc/chrony.conf server pool.ntp.org iburst driftfile /var/lib/chrony/drift makestep 1.0 3 rtcsync keyfile /etc/chrony.keys leapsectz right/UTC logdir /var/log/chrony Restart to pick up new config $ sudo systemctl restart chronyd $ sudo systemctl status chronyd * chronyd.service - NTP client/server Loaded: loaded (/usr/lib/systemd/system/chronyd.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2023-02-17 15:47:28 EST; 20h ago Docs: man:chronyd(8) man:chrony.conf(5) Process: 798 ExecStartPost=/usr/libexec/chrony-helper update-daemon (code=exited, status=0/SUCCESS) Process: 789 ExecStart=/usr/sbin/chronyd $OPTIONS (code=exited, status=0/SUCCESS) Main PID: 796 (chronyd) Tasks: 1 (limit: 11366) Memory: 2.3M CGroup: /system.slice/chronyd.service \u2514\u2500796 /usr/sbin/chronyd Test $ chronyc sources -v .-- Source mode \u0027^\u0027 = server, \u0027=\u0027 = peer, \u0027#\u0027 = local clock. / .- Source state \u0027*\u0027 = current best, \u0027+\u0027 = combined, \u0027-\u0027 = not combined, | / \u0027x\u0027 = may be in error, \u0027~\u0027 = too variable, \u0027?\u0027 = unusable. || .- xxxx [ yyyy ] +/- zzzz || Reachability register (octal) -. | xxxx = adjusted offset, || Log2(Polling interval) --. | | yyyy = measured offset, || \\ | | zzzz = estimated error. || | | \\ MS Name/IP address Stratum Poll Reach LastRx Last sample == ============================================================================ ^? your-ip-name-d\u003e 0 8 0 - +0ns[ +0ns] +/- 0ns ... $ timedatectl Local time: Wed 2023-07-12 09:03:57 EDT Universal time: Wed 2023-07-12 13:03:57 UTC RTC time: Wed 2023-07-12 13:03:57 Time zone: America/New_York (EDT, -0400) System clock synchronized: yes NTP service: active RTC in local TZ: no ... $ systemctl is-active chronyd.service active ... $ chronyc tracking Reference ID : 404F64C5 (ntpool1.258.ntp.org) Stratum : 3 Ref time (UTC) : Wed Jul 12 12:47:34 2023 System time : 0.000000002 seconds slow of NTP time Last offset : +1.114915133 seconds RMS offset : 1.114915133 seconds Frequency : 32.362 ppm slow Residual freq : +22.860 ppm Skew : 3.901 ppm Root delay : 0.046558209 seconds Root dispersion : 0.050582517 seconds Update interval : 0.0 seconds Leap status : Normal Reference: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_basic_system_settings/using-chrony-to-configure-ntp_configuring-basic-system-settings Debian Debian uses systemd-timesyncd service. $ sudo systemctl status systemd-timesyncd * systemd-timesyncd.service - Network Time Synchronization Loaded: loaded (/lib/systemd/system/systemd-timesyncd.service; enabled; vendor preset: enabled) Drop-In: /lib/systemd/system/systemd-timesyncd.service.d \u2514\u2500disable-with-time-daemon.conf Active: active (running) since Sun 2022-07-24 12:06:36 EDT; 2 weeks 3 days ago Docs: man:systemd-timesyncd.service(8) Main PID: 559 (systemd-timesyn) Status: \"Synchronized to time server for the first time 192.155.94.72:123 (2.debian.pool.ntp.org).\" Tasks: 2 (limit: 951) Memory: 1.0M CGroup: /system.slice/systemd-timesyncd.service \u2514\u2500559 /lib/systemd/systemd-timesyncd Reference: Debian: https://www.debian.org/doc/manuals/debian-handbook/sect.config-misc.en.html#sect.time-synchronization Ubuntu: https://ubuntu.com/server/docs/network-ntp E-Mail - Client for Sending Local Mail Identify Mail Client Host and Domain Edit the following files: /etc/hostname (add fully qualified host \u0026 domain; i.e.: www.example.com) /etc/mailname (add domain; i.e.: example.com) Mail Transport Agent (MTA) packages Install an SMTP daemon to transfer mail to the E-Mail server. Debian Install postfix $ sudo apt-get install postfix Reconfigure postfix, if it does not pop up, and select satellite system. $ sudo dpkg-reconfigure postfix The following assumes your host is named app and your email server is smtp.\u003cdomain\u003e File: /etc/postfix/main.cf # See /usr/share/postfix/main.cf.dist for a commented, more complete version # Debian specific: Specifying a file name will cause the first # line of that file to be used as the name. The Debian default # is /etc/mailname. #myorigin = /etc/mailname smtpd_banner = $myhostname ESMTP $mail_name (Debian/GNU) biff = no # appending .domain is the MUA\u0027s job. append_dot_mydomain = no # Uncomment the next line to generate \"delayed mail\" warnings #delay_warning_time = 4h readme_directory = no # See http://www.postfix.org/COMPATIBILITY_README.html -- default to 3.6 on # fresh installs. compatibility_level = 3.6 # TLS parameters smtpd_tls_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem smtpd_tls_key_file=/etc/ssl/private/ssl-cert-snakeoil.key smtpd_tls_security_level=may smtp_tls_CApath=/etc/ssl/certs smtp_tls_security_level=may smtp_tls_session_cache_database = btree:${data_directory}/smtp_scache smtpd_relay_restrictions = permit_mynetworks permit_sasl_authenticated defer_unauth_destination myhostname = app alias_maps = hash:/etc/aliases alias_database = hash:/etc/aliases myorigin = /etc/mailname mydestination = app.example.com, $myhostname, app, localhost.localdomain, localhost relayhost = smtp.example.com mynetworks = 127.0.0.0/8 [::ffff:127.0.0.0]/104 [::1]/128 mailbox_size_limit = 0 recipient_delimiter = + inet_interfaces = loopback-only inet_protocols = all Check postfix systemd service. $ sudo systemctl status postfix \u25cf postfix.service - Postfix Mail Transport Agent Loaded: loaded (/lib/systemd/system/postfix.service; enabled; preset: enabled) Active: active (exited) since Sat 2023-07-22 09:32:00 EDT; 5h 26min ago Docs: man:postfix(1) Process: 1178 ExecStart=/bin/true (code=exited, status=0/SUCCESS) Main PID: 1178 (code=exited, status=0/SUCCESS) CPU: 1ms Jul 22 09:32:00 app.example.com systemd[1]: Starting postfix.service - Postfix Mail Transport Agent... Jul 22 09:32:00 app.example.com systemd[1]: Finished postfix.service - Postfix Mail Transport Agent. RedHat Install postfix $ sudo dnf install postfix The following assumes your host is named app and your email server is smtp.\u003cdomain\u003e File: /etc/postfix/main.cf smtpd_banner = $myhostname ESMTP $mail_name (Linux) biff = no # appending .domain is the MUA\u0027s job. append_dot_mydomain = no # Uncomment the next line to generate \"delayed mail\" warnings #delay_warning_time = 4h readme_directory = no # See http://www.postfix.org/COMPATIBILITY_README.html -- default to 2 on # fresh installs. compatibility_level = 2 # TLS parameters smtpd_tls_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pem smtpd_tls_key_file=/etc/ssl/private/ssl-cert-snakeoil.key smtpd_use_tls=yes smtpd_tls_session_cache_database = btree:${data_directory}/smtpd_scache smtp_tls_session_cache_database = btree:${data_directory}/smtp_scache # See /usr/share/doc/postfix/TLS_README.gz in the postfix-doc package for # information on enabling SSL in the smtp client. smtpd_relay_restrictions = permit_mynetworks permit_sasl_authenticated defer_unauth_destination myhostname = app.example.com alias_maps = hash:/etc/aliases alias_database = hash:/etc/aliases mydestination = app.example.com, localhost.example.com, localhost relayhost = smtp.example.com mynetworks = 127.0.0.0/8 [::ffff:127.0.0.0]/104 [::1]/128 192.168.1.0/32 mailbox_size_limit = 0 recipient_delimiter = + inet_interfaces = loopback-only inet_protocols = all Check the status of postfix $ sudo systemctl status postfix [sudo] password for don: \u25cf postfix.service - Postfix Mail Transport Agent Loaded: loaded (/usr/lib/systemd/system/postfix.service; enabled; preset: disabled) Active: active (running) since Wed 2023-06-07 17:43:43 EDT; 22h ago Main PID: 2182 (master) Tasks: 3 (limit: 99462) Memory: 8.6M CPU: 1.975s CGroup: /system.slice/postfix.service \u251c\u2500 2182 /usr/libexec/postfix/master -w \u251c\u2500 2184 qmgr -l -t unix -u \u2514\u250015581 pickup -l -t unix -u MAC OS send mail to local server, not system configured in mail app Use the IP address of the local mail server, or you can edit /etc/hosts and use that name. Postfix does not run as a daemon, but is run by the SMTP process, probably fired of by a listener for port 25. File: /etc/postfix/main.cf % sudo vi /etc/postfix/main.cf ~ myhostname = square.example.com ~ mydomain = example.com ~ relayhost = [192.168.1.3] Test mail First install the command line mail interface(s). I use mail and mutt. Debian :::Text $ sudo apt-get install mailutils mutt RedHat :::Text $ sudo dnf install s-nail mutt % mail -s \"Hello internal mail\" don@example.com \u003c/dev/null Null message body; hope that\u0027s ok Shows up as: don@square.example.com ... mutt -s \"Hello internal mail from mutt\" don@example.com Shows up as: don@square.local Mutt change from address File: ~/.muttrc set from=\"Square \u003cdon@square.example.com\u003e\" set hostname=\"square.example.com\" Shows up as: don@square.example.com Update root destination in aliases File: /etc/aliases ~ # Person who should get root\u0027s mail #root: marc root: bob@example.com ~ Update aliases into database format $ sudo newaliases Create mail script to set variables File: ~/mail.sh #!/bin/bash ####################################################################### # # File: mail.sh # # Usage: mail.sh \u003cFile Name to Mail\u003e \u003cSubject\u003e # Change the REPLYTO, FROM, and MAILTO variables # and choose RedHat or Debian # # Who When Why # --------- ----------- ----------------------------------------------- # D. Cohoon Feb-2023 VPS host name cannot be changed, so set headers ####################################################################### function usage () { /usr/bin/echo \"Usage: ${0} \u003cFile Name to Mail\u003e \u003cSubject\u003e\" exit 1 } #------------------ if [ $# -lt 2 ]; then usage fi # if [ ! -z ${1} ] \u0026\u0026 [ ! -f ${1} ]; then usage fi # #------------------ HOSTNAME=$(hostname -s) DOMAINNAME=$(hostname -d) FILE=${1} # First arg shift 1 SUBJECT=\"${HOSTNAME}.${DOMAINNAME}:${@}\" # Remainder of args # #------------------ export REPLYTO=root@app.example.com FROM=root@app.example.com #FROM=\"${HOSTNAME}@${DOMAINNAME}\" MAILTO=bob@example.com # #------------------ # Debian: install mailutils #/usr/bin/cat ${FILE} | /usr/bin/mail -aFROM:${FROM} -s \"${SUBJECT}\" ${MAILTO} # RedHat: install s-nail #/usr/bin/cat ${FILE} | /usr/bin/mail --from-address=${FROM} -s \"${SUBJECT}\" ${MAILTO} Monit - Monitor System and Restart Processes Monit is a small Open Source utility for managing and monitoring Unix systems. Monit conducts automatic maintenance and repair and can execute meaningful causal actions in error situations. Reference: https://mmonit.com/monit/ Installation The Debian Way $ sudo apt-get install monit The RedHat Way $ sudo dnf install monit Configuration Change the mailserver to yours, and add some general monitoring. File: /etc/monit/monitrc: # Mail server set mailserver www.example.com port 25 # primary mailserver # Don 28-Dec 2021 - general monitoring check system $HOST if loadavg (1min) per core \u003e 2 for 5 cycles then alert if loadavg (5min) per core \u003e 1.5 for 10 cycles then alert if cpu usage \u003e 95% for 5 cycles then alert if memory usage \u003e 90% then alert if swap usage \u003e 50% then alert check device root with path / if space usage \u003e 90% then alert if inode usage \u003e 90% then alert if changed fsflags then alert if service time \u003e 250 milliseconds for 5 cycles then alert if read rate \u003e 500 operations/s for 5 cycles then alert if write rate \u003e 200 operations/s for 5 cycles then alert check network eth0 with interface eth0 if failed link then alert if changed link then alert if saturation \u003e 90% for 2 cycles then alert if download \u003e 10 MB/s for 5 cycles then alert if total uploaded \u003e 1 GB in last hour then alert check host REACHABILITY with address 1.1.1.1 if failed ping with timeout 10 seconds then alert Process Monitor and restart the ssh process (and others that you may need using this as a guide). File: /etc/monit/conf.d/sshd check process sshd with pidfile /var/run/sshd.pid alert root@example.com with mail-format { from: monit@example.com subject: monit alert: $SERVICE $EVENT $DATE message: $DESCRIPTION } start program \"/etc/init.d/ssh start\" stop program \"/etc/init.d/ssh stop\" Munin - Resource History Monitor Munin is a networked resource monitoring tool (started in 2002) that can help analyze resource trends and what just happened to kill our performance? problems. It is designed to be very plug and play. A default installation provides a lot of graphs with almost no work. Requires Apache or nginx for graphs. Reference: http://guide.munin-monitoring.org/en/latest/tutorial/alert.html Installation On all nodes The Debian Way package libdb-pg-perl is required for postgresql # sudo apt-get install munin libdbd-pg-perl The RedHat Way package perl-DBD-Pg is required for postgresql # sudo dnf install munin perl-DBD-Pg On Munin-Master node, add the following list of hosts to monitor: File: /etc/munin.munin.conf ~ # Local Host [app.example.com] address 127.0.0.1 use_node_name yes # E-Mail host [www.example.com] address 192.168.1.3 use_node_name yes ~ The Debian (ufw) Way On Munin-Node node, allow master into IPv4/6 port: $ sudo ufw allow 4949 $ sudo ufw status | grep 4949 4949 ALLOW Anywhere 4949 (v6) ALLOW Anywhere (v6) IPv4 and IPv6 The RedHat (firewall-cmd) Way On Munin-Node node, allow master into IPv4 port: $ sudo firewall-cmd --permanent --zone=public --add-port=4949/tcp $ sudo firewall-cmd --reload $ sudo firewall-cmd --permanent --list-ports 4949/tcp IPv4 On Munin-Node node, add the Munin-Master IP address to the following: File: /etc/munin/munin-node.conf ~ # A list of addresses that are allowed to connect. This must be a # regular expression, since Net::Server does not understand CIDR-style # network notation unless the perl module Net::CIDR is installed. You # may repeat the allow line as many times as you\u0027d like allow ^127\\.0\\.0\\.1$ allow ^::1$ allow ^192\\.168\\.1\\.3$ allow ^fe80::abcd:1234:0000:abcd$ ~ Check your munin-node functions from command line using the network cat utility Debian -\u003e $ sudo apt-get install ncat: RedHat -\u003e $ sudo dnf install ncat: Try comands: list nodes config fetch version quit quit Examples: $ ncat 127.0.0.1 4949 # munin node at app.example.com list acpi apache_accesses apache_processes apache_volume cpu df df_inode entropy forks fw_packets http_loadtime if_em1 if_eno1 if_err_em1 if_err_eno1 if_err_tun0 if_err_wlp58s0 if_tun0 if_wlp58s0 interrupts irqstats load lpstat memory munin_stats netstat ntp_198.23.200.19 ntp_208.94.243.142 ntp_216.218.254.202 ntp_91.189.94.4 ntp_96.126.100.203 ntp_kernel_err ntp_kernel_pll_freq ntp_kernel_pll_off ntp_offset ntp_states open_files open_inodes postfix_mailqueue postfix_mailvolume postgres_autovacuum postgres_bgwriter postgres_cache_ALL postgres_cache_twotree postgres_checkpoints postgres_connections_ALL postgres_connections_db postgres_connections_twotree postgres_locks_ALL postgres_locks_twotree postgres_querylength_ALL postgres_querylength_twotree postgres_scans_twotree postgres_size_ALL postgres_size_twotree postgres_transactions_ALL postgres_transactions_twotree postgres_tuples_twotree postgres_users postgres_xlog proc_pri processes swap threads uptime users vmstat . fetch df _dev_nvme0n1p2.value 34.4721606114914 _dev_shm.value 0.000540811610733636 _run.value 0.183629672785198 _run_lock.value 0.15625 _run_qemu.value 0 _dev_sda1.value 39.2182617590549 _dev_nvme0n1p1.value 1.02513530868728 _dev_sdb1.value 29.1143742265263 . fetch cpu user.value 3392679 nice.value 310628 system.value 792413 idle.value 49254331 iowait.value 202415 irq.value 0 softirq.value 56281 steal.value 0 guest.value 0 Apache monitoring requires the mod_status to be enabled and add your IP address range to the status.conf. Enable apache module mod_status: $ sudo a2enmod status Check the IP addresses in the apache status configuration. Change Require ip \u003caddress\u003e to allow other IP addresses to connect to the munin monitor. File: /etc/apache2/mods-enabled/status.conf ~ \u003cLocation /server-status\u003e SetHandler server-status Require local Require ip 192.168.1.0/24 #Require ip 192.0.2.0/24 \u003c/Location\u003e ~ Check apache plugin: $ sudo munin-run apache_volume volume80.value 500736 Check postgresql plugin: $ sudo munin-run postgres_connections_miniflux active.value 0 idle.value 1 idletransaction.value 0 unknown.value 0 waiting.value 0 Check the munin-node daemon status: $ sudo systemctl status munin-node Check the munin-master daemon status: $ sudo systemctl status munin The utility munin-node-configure is used by the Munin installation procedure to check which plugins are suitable for your node and create the links automatically. It can be called every time when a system configuration changes (services, hardware, etc) on the node and it will adjust the collection of plugins accordingly. \u0027-shell\u0027 will display new configuration plugin links \u0027ln -s ...\u0027 for you. For instance, below a new network interface (if) was discovered since the last configuration of munin. To enable the new monitoring simply execute the \u0027ln -s ...\u0027 commands to create soft links, so interface veth2e40fe9 will be monitored. $ sudo munin-node-configure -shell ln -s \u0027/usr/share/munin/plugins/if_\u0027 \u0027/etc/munin/plugins/if_veth2e40fe9\u0027 ln -s \u0027/usr/share/munin/plugins/if_err_\u0027 \u0027/etc/munin/plugins/if_err_veth2e40fe9\u0027 To have munin-node-configure display \u0027rm ...\u0027 commands for plugins with software that may no longer be installed, use the option \u2018\u2013remove-also\u2019. $ sudo munin-node-configure -shell -remove-also ln -s \u0027/usr/share/munin/plugins/if_\u0027 \u0027/etc/munin/plugins/if_veth2e40fe9\u0027 rm -f \u0027/etc/munin/plugins/if_veth0049d71\u0027 ln -s \u0027/usr/share/munin/plugins/if_err_\u0027 \u0027/etc/munin/plugins/if_err_veth2e40fe9\u0027 rm -f \u0027/etc/munin/plugins/if_err_veth0049d71\u0027 Enabled monitors can be found in the same location $ ls -l /etc/munin/plugins | grep apache lrwxrwxrwx 1 root root 40 Jun 30 2018 apache_accesses -\u003e /usr/share/munin/plugins/apache_accesses lrwxrwxrwx 1 root root 41 Jun 30 2018 apache_processes -\u003e /usr/share/munin/plugins/apache_processes lrwxrwxrwx 1 root root 38 Jun 30 2018 apache_volume -\u003e /usr/share/munin/plugins/apache_volume Testing new plugins has an autoconf option to munin-run. Errors will be displayed, and a debug \u0027-d\u0027 option is also available. $ sudo munin-run postgres_connections_miniflux autoconf yes $ sudo munin-run -d postgres_connections_miniflux autoconf # Running \u0027munin-run\u0027 via \u0027systemd-run\u0027 with systemd properties based on \u0027munin-node.service\u0027. # Command invocation: systemd-run --collect --pipe --quiet --wait --property EnvironmentFile=/tmp/YRfAa1dq9U --property UMask=0022 --property LimitCPU=infinity --property LimitFSIZE=infinity --property LimitDATA=infinity --property LimitSTACK=infinity --property LimitCORE=infinity --property LimitRSS=infinity --property LimitNOFILE=524288 --property LimitAS=infinity --property LimitNPROC=14150 --property LimitMEMLOCK=65536 --property LimitLOCKS=infinity --property LimitSIGPENDING=14150 --property LimitMSGQUEUE=819200 --property LimitNICE=0 --property LimitRTPRIO=0 --property LimitRTTIME=infinity --property SecureBits=0 --property \u0027CapabilityBoundingSet=cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend cap_audit_read cap_perfmon cap_bpf cap_checkpoint_restore\u0027 --property AmbientCapabilities= --property DynamicUser=no --property MountFlags= --property PrivateTmp=yes --property PrivateDevices=no --property ProtectClock=no --property ProtectKernelTunables=no --property ProtectKernelModules=no --property ProtectKernelLogs=no --property ProtectControlGroups=no --property PrivateNetwork=no --property PrivateUsers=no --property PrivateMounts=no --property ProtectHome=yes --property ProtectSystem=full --property NoNewPrivileges=no --property LockPersonality=no --property MemoryDenyWriteExecute=no --property RestrictRealtime=no --property RestrictSUIDSGID=no --property RestrictNamespaces=no --property ProtectProc=default --property ProtectHostname=no -- /usr/sbin/munin-run --ignore-systemd-properties -d postgres_connections_miniflux autoconf # Processing plugin configuration from /etc/munin/plugin-conf.d/README # Processing plugin configuration from /etc/munin/plugin-conf.d/dhcpd3 # Processing plugin configuration from /etc/munin/plugin-conf.d/munin-node # Processing plugin configuration from /etc/munin/plugin-conf.d/spamstats # Setting /rgid/ruid/ to /130/117/ # Setting /egid/euid/ to /130 130/117/ # Setting up environment # Environment PGPORT = 5432 # Environment PGUSER = postgres # About to run \u0027/etc/munin/plugins/postgres_connections_miniflux autoconf\u0027 yes Alert via e-mail: Reference: https://guide.munin-monitoring.org/en/latest/tutorial/alert.html#alerts-send-by-local-system-tools Change your email here File: /etc/munin/munin.conf ~ contact.email.command mail -s \"Munin-notification for ${var:group} :: ${var:host}\" your@email.address.here ~ Adjust disk full thresholds: Adjust this in master /etc/munin.conf section for node File: /etc/munin.conf [beaglebone] address 192.168.1.7 use_node_name yes diskstats_latency.mmcblk0.avgrdwait.warning 0:10 diskstats_latency.mmcblk0.avgrdwait.critical -5:5 diskstats_latency.mmcblk0.avgwdwait.warning 0:10 diskstats_latency.mmcblk0.avgwdwait.critical -5:5 diskstats_latency.mmcblk0.avgwait.warning 0:10 diskstats_latency.mmcblk0.avgwait.critical -5:5 Graph Example Redhat Alternative Cockpit $ sudo systemctl start Cockpit To log in to Cockpit, open your web browser to localhost:9090 and enter your Linux username and password. Reference: https://www.redhat.com/sysadmin/intro-cockpit Fail2ban - Automatic Firewall Blocking Daemon to ban hosts that cause multiple authentication errors by monitoring system logs. Reference: https://github.com/fail2ban/fail2ban Install The Debian Way $ sudo apt-get install fail2ban The RedHat Way $ sudo dnf install fail2ban Configure Create a jail.local file to override the defaults. Update your email and IP addresses to suit your environment. Also add or disable applications you do not run. See the reference above for example of how to do that. action = %(action_)s This defines the action to execute when a limit is reached. By default it will only block the user. To receive an email at each ban, set it to: action = %(action_mw) To receive the logs with the mail, set it to: action = %(action_mwl) File: /etc/fail2ban/jail.local [DEFAULT] # email destemail = don@example.com sender = root@example.com # ban \u0026 send an e-mail with whois report and relevant log lines # to the destemail. action = %(action_mwl)s # whitelist ignoreip = 127.0.0.1 192.168.1.0/24 8.8.8.8 1.1.1.1 Secure sshd File: /etc/fail2ban/jail.d/sshd.local The Debian Way [sshd] enabled = true port = 22 filter = sshd action = iptables-multiport[name=sshd, port=\"ssh\"] logpath = /var/log/auth.log maxretry = 3 bantime = 1d The RedHat Way [sshd] enabled = true port = 22 filter = sshd action = iptables-multiport[name=sshd, port=\"ssh\"] logpath = /var/log/secure maxretry = 3 bantime = 1d More filters show which daemons are available to be enabled here: /etc/fail2ban/filter.d/ Backup - Save Your Files Daily To find the proper name of your USB stick, check the current mounts: $ sudo lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 1.8T 0 disk \u2514\u2500sda1 8:1 0 1.8T 0 part sdb 8:16 1 0B 0 disk sdc 8:32 1 0B 0 disk nvme0n1 259:0 0 238.5G 0 disk \u251c\u2500nvme0n1p1 259:1 0 300M 0 part /boot/efi \u251c\u2500nvme0n1p2 259:2 0 119.2G 0 part / \u251c\u2500nvme0n1p3 259:3 0 16.9G 0 part [SWAP] Plug it is, then check dmesg -x immediately after plugging it in. Look for : [201373.210797] sdd: sdd1 [201373.211917] sd 2:0:0:0: [sdd] Attached SCSI removable disk Then run blkid again. You can see the new entry, sdd. $ sudo lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 1.8T 0 disk \u2514\u2500sda1 8:1 0 1.8T 0 part sdb 8:16 1 0B 0 disk sdc 8:32 1 0B 0 disk sdd 8:48 1 28.9G 0 disk \u003c----- New USB Stick nvme0n1 259:0 0 238.5G 0 disk \u251c\u2500nvme0n1p1 259:1 0 300M 0 part /boot/efi \u251c\u2500nvme0n1p2 259:2 0 119.2G 0 part / \u251c\u2500nvme0n1p3 259:3 0 16.9G 0 part [SWAP] Automatic backup to USB disk Format new USB stick. Here are the commands to fdisk: m - menu p - print existing partitions d - delete partition n - create new partition (in this case only a primary partition is required) w - write partition q - quit ::Text $ sudo fdisk /dev/sdd Check the USB stick label and filesystem (this example has no filesystem) $ sudo blkid /dev/sdd1 /dev/sdd1: PARTUUID=\"66bc7da7-1234-abcd-1234-ea4bfe7e00a7\" So create an ext4 filesystem on the new partition(1) $ sudo mkfs.ext4 /dev/sdd1 mke2fs 1.46.2 (28-Feb-2021) /dev/sdc1 contains a vfat file system Proceed anyway? (y,N) y Creating filesystem with 7566075 4k blocks and 1892352 inodes Filesystem UUID: 8e33672c-1283-49de-98b8-6fd841372db6 Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000 Allocating group tables: done Writing inode tables: done Creating journal (32768 blocks): done Writing superblocks and filesystem accounting information: done Check the new filesystem, now we see it is TYPE=\"ext4\" $ sudo blkid /dev/sdd1 /dev/sdc1: UUID=\"8e33672c-1234-49de-abcd-6fd841372db6\" BLOCK_SIZE=\"4096\" TYPE=\"ext4\" PARTUUID=\"66bc7da7-c9c3-4342-8073-ea4bfe7e00a7\" Label the USB stick as \u0027backup\u0027 so autobackup can find and mount it, then verify that LABEL=\"backup\" $ sudo e2label /dev/sdd1 backup $ sudo blkid /dev/sdd1 /dev/sdd1: LABEL=\"backup\" UUID=\"8e33672c-1234-49de-abcd-6fd841372db6\" BLOCK_SIZE=\"4096\" TYPE=\"ext4\" PARTUUID=\"66bc7da7-c9c3-4342-8073-ea4bfe7e00a7\" Copy the autobackup software onto this server $ git clone https://github.com/bablokb/autobackup-service Install the autobackup software $ cd autobackup-service/ $ sudo ./tools/install RedHat Changes Lines: 21, 30, 31 File: ./tools/install 17 check_packages() { 18 local p 19 for p in \"$@\"; do 20 echo -en \"Checking $p ... \" \u003e\u00262 21 rpm -qa \"$p\" 2\u003e/dev/null | grep -q \"Status.*ok\" || return 0 22 echo \"ok\" \u003e\u00262 23 done 24 return 1 25 } 26 27 install_packages() { 28 if [ -n \"$PACKAGES\" ] \u0026\u0026 check_packages $PACKAGES; then 29 echo -e \"[INFO] installing additional packages\" 2\u003e\u00261 30 dnf update 31 dnf -y --no-upgrade install $PACKAGES 32 fi 33 } Edit the autobackup configuration file, assigning LABEL=backup, and other items below: File: /etc/autobackup.conf ~ # =\u003e File: /etc/autobackup.conf \u003c= # label of backup partition LABEL=backup # write messages to syslog SYSLOG=1 # wait for device to appear (in seconds) WAIT_FOR_DEVICE=2 # run a backup on every mount (i.e. multiple daily backups) force_daily=0 # backup-levels - this must match your entries in /etc/rsnapshot.conf, i.e. # you must have a corresponding \u0027retain\u0027 or \u0027interval\u0027 entry. # The autobackup-script will skip empty levels daily=\"day\" weekly=\"week\" monthly=\"month\" yearly=\"\" \"/etc/autobackup.conf\" line 29 of 29 --100%-- Edit the rsnapshot configuration file, be sure to use TABS in the BACKUP POINTS / SCRIPTS section. File: /etc/rsnapshot.conf ~ # =\u003e File /etc/rsnapshot.conf \u003c= ########################### # SNAPSHOT ROOT DIRECTORY # ########################### # All snapshots will be stored under this root directory. # #snapshot_root /var/cache/rsnapshot/ snapshot_root /tmp/autobackup/.autobackup/ ~ ~ ######################################### # BACKUP LEVELS / INTERVALS # # Must be unique and in ascending order # # e.g. alpha, beta, gamma, etc. # ######################################### retain day 7 retain week 4 retain month 3 #retain year 3 ############################### ### BACKUP POINTS / SCRIPTS ### ############################### # LOCALHOST # backup /etc/ ./ # backup /var/backups/ ./ # backup /usr/local/ ./ # backup /home ./ # NOTE: Use tabs! # LOCALHOST$ backup^I/etc/^I./$ backup^I/var/backups/^I./$ backup^I/usr/local/^I./$ backup^I/home^I^I./$ ~ \u003e \"/etc/rsnapshot.conf\" Copy autobackup script from install to your home directory cp autobackup-service/files/usr/local/sbin/autobackup $HOME/autobackup-service/autobackup.sh Comment out lines 58 through 61 from \"\u003c\" to \"\u003e\" below, to allow running in cron. autobackup-service normally runs automatically when a USB stick with the proper label is inserted into the machine. Comment out the if statement to allow it to run by cron. File: $HOME/autobackup-service/autobackup.sh 58,61c60,64 \u003c if [ \"${DEVICE:5:3}\" != \"$udev_arg\" ]; then \u003c msg \"info: partition with label $LABEL is not on newly plugged device $udev_arg\" \u003c exit 0 \u003c fi --- \u003e # Don -\u003e do not check, as we are screduling through cron \u003e # if [ \"${DEVICE:5:3}\" != \"$udev_arg\" ]; then \u003e # msg \"info: partition with label $LABEL is not on newly plugged device $udev_arg\" \u003e # exit 0 \u003e # fi Schedule in /etc/cron.d (change your home directory): File: /etc/cron.d/autobackup-daily # This is a cron file for autobackup/rsnapshot. # 0 */4 * * * root /usr/bin/rsnapshot alpha # 30 3 * * * root /usr/bin/rsnapshot beta # 0 3 * * 1 root /usr/bin/rsnapshot gamma # 30 2 1 * * root /usr/bin/rsnapshot delta SHELL=/bin/bash PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin MAILTO=\"don@example.com\" # m h dom mon dow user command 55 12 * * * root /home/don/autobackup-service/autobackup.sh Log entries will be in the syslog $ sudo grep autobackup.sh /var/log/syslog Jan 23 12:51:11 box autobackup.sh: info: LABEL = backup Jan 23 12:51:11 box autobackup.sh: info: WAIT_FOR_DEVICE = 2 Jan 23 12:51:11 box autobackup.sh: info: force_daily = 0 Jan 23 12:51:11 box autobackup.sh: info: yearly = Jan 23 12:51:11 box autobackup.sh: info: monthly = month Jan 23 12:51:11 box autobackup.sh: info: weekly = week Jan 23 12:51:11 box autobackup.sh: info: daily = day Jan 23 12:51:13 box autobackup.sh: info: checking: Jan 23 12:51:13 box autobackup.sh: info: mount-directory: /tmp/autobackup Jan 23 12:51:13 box autobackup.sh: info: current year: 2021 Jan 23 12:51:13 box autobackup.sh: info: current month: 01 Jan 23 12:51:13 box autobackup.sh: info: current week: 03 Jan 23 12:51:13 box autobackup.sh: info: current day: 023 Jan 23 12:51:13 box autobackup.sh: info: starting backup for interval: month (last backup: 0) Jan 23 12:51:13 box autobackup.sh: info: starting backup for interval: week (last backup: 0) Jan 23 12:51:13 box autobackup.sh: info: starting backup for interval: day (last backup: 0) Jan 23 12:51:15 box autobackup.sh: info: umounting /dev/sda1 Automatic Backup of PostgreSQL Database Place a script in /etc/cron.daily and it will be run once a day, using the root account. cron.daily To check the times look here: The Debian Way $ grep run-parts /etc/crontab 17 * * * * root cd / \u0026\u0026 run-parts --report /etc/cron.hourly 25 6 * * * root test -x /usr/sbin/anacron || ( cd / \u0026\u0026 run-parts --report /etc/cron.daily ) 47 6 * * 7 root test -x /usr/sbin/anacron || ( cd / \u0026\u0026 run-parts --report /etc/cron.weekly ) 52 6 1 * * root test -x /usr/sbin/anacron || ( cd / \u0026\u0026 run-parts --report /etc/cron.monthly ) So our daily runs start at 6:25am every day. The RedHat Way # cat /etc/anacrontab # /etc/anacrontab: configuration file for anacron # See anacron(8) and anacrontab(5) for details. SHELL=/bin/sh PATH=/sbin:/bin:/usr/sbin:/usr/bin MAILTO=root # the maximal random delay added to the base delay of the jobs RANDOM_DELAY=45 # the jobs will be started during the following hours only START_HOURS_RANGE=3-22 #period in days delay in minutes job-identifier command 1 5 cron.daily nice run-parts /etc/cron.daily 7 25 cron.weekly nice run-parts /etc/cron.weekly @monthly 45 cron.monthly nice run-parts /etc/cron.monthly So our daily runs start ... any idea? Backing up a PostrgreSQL database can be done while everything is up and running with the following script. Backups are stored in /data/backups. File: /etc/cron.daily/backup_nextcloud #!/bin/bash LOGFILE=/var/log/backup_db.log ID=$(id -un) if [ ${ID} != \"root\" ]; then echo \"Must run as root, try sudo\" exit 1 fi # echo $(date) ${0} \u003e\u003e $LOGFILE umask 027 export DATA=/data/backups if cd ${DATA}; then # Postgres #/usr/bin/pg_dump -c nextcloud \u003e $DATA/nextcloud.db.$(date +%j) \u003c/dev/null sudo -u postgres /usr/bin/pg_dump -c nextcloud \u003e $DATA/nextcloud.db \u003c/dev/null date \u003e\u003e $LOGFILE sync sync sync sync savelog -c 7 nextcloud.db \u003e\u003e$LOGFILE 2\u003e\u00261 fi Rsync - Remote File Synchronization Rsync is a good way to keep a daily backup as it only copies changed files to the destination. Make sure you use a separate disk and preferably separate system, as rsync works great over the network. The PostgreSQL backup above should be sent off to another system using this method. Another rsync script should be called by PostgreSQL backup to do the database network backup. Just copy this one, change the directories, and call it at the end of the database backup. Schedule This cron entry will run at 8:40am every day by user root. File: /etc/cron.d/rsync # This is a cron file for rsync to NAS SHELL=/bin/bash PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin MAILTO=\"don@example.com\" # m h dom mon dow user command 40 8 * * * root /mnt/raid1/rsync.sh noask Script This script will backup the local directory, /mnt/raid1/data, to a remote system (IP Address 192.168.1.2). The files on the remote system will be at /mnt/vol09/backups. The first run will copy everything, all next runs will only copy changed files. Any files deleted on the source will also be deleted on the destination. To schedule in cron the parameter \u0027noask\u0027 is used, as shown above. Otherwise there is a prompt for y/n. The last run\u0027s history is in log file /mnt/raid1/rsync.log. File: /mnt/raid1/rsync.sh #!/bin/bash DIR=/mnt/raid1 LOG=${DIR}/rsync.log cd ${DIR} date \u003e${LOG} ASK=${1} if [ -z ${ASK} ]; then echo \"Asking\" fi # if [ -z ${ASK} ]; then echo -n \"Copy data? y/n: \" read askme if [[ $askme =~ ^[Yy]$ ]]; then rsync -avzz --ignore-errors --progress --delete ${DIR}/data root@192.168.1.2:/mnt/vol09/backups/ |tee -a ${LOG} else echo \"Sync of data skipped\" echo \". . .\" fi else rsync -avzz --ignore-errors --progress --delete ${DIR}/data root@192.168.1.2:/mnt/vol09/backups/ |tee -a ${LOG} fi # date \u003e\u003e${LOG} Logwatch - Daily Alert of Logging Activity Logwatch is a customizable, pluggable log-monitoring system. It will go through your logs for a given period of time and make a report in the areas that you wish with the detail that you wish. Logwatch is being used for Linux and many types of UNIX. Installation Debian: $ sudo apt-get install logwatch Redhat: $ sudo dnf install logwatch Schedule File: /etc/cron.daily/00logwatch #!/bin/bash #Check if removed-but-not-purged test -x /usr/share/logwatch/scripts/logwatch.pl || exit 0 #execute #/usr/sbin/logwatch --output mail /usr/sbin/logwatch --mailto don@example.com #Note: It\u0027s possible to force the recipient in above command #Just pass --mailto address@a.com instead of --output mail Add services You can add iptables summary on the daily report. It shows which IP addresses have been blocked by UFW. $ sudo cp /usr/share/logwatch/default.conf/services/iptables.conf /etc/logwatch/conf/services/ You may need to add syslog on Ubuntu servers File: /etc/logwatch/conf/services/iptables.conf The Debian Way ~ # Which logfile group... LogFile = syslog ~ Logcheck - mails anomalies in the system logfiles to the admin The logcheck program helps spot problems and security violations in your logfiles automatically and will send the results to you periodically in an e-mail. By default logcheck runs as an hourly cronjob just off the hour and after every reboot. Installation The Debian Way $ sudo apt-get install logcheck The RedHat Way $ sudo dnf install epel-release \u0027dnf-command(copr)\u0027 $ sudo dnf copr enable brianjmurrell/epel-8 $ sudo dnf install logcheck $ sudo setfacl -R -m u:logcheck:rx /var/log/secure* $ sudo setfacl -R -m u:logcheck:rx /var/log/messages* $ sudo dnf copr disable brianjmurrell/epel-8 Schedule Normally the package installation will schedule a cron job for you. Check it here: File: /etc/cron.d/logcheck # Cron job runs at 2 minutes past every hour # /etc/cron.d/logcheck: crontab entries for the logcheck package PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin MAILTO=root @reboot logcheck if [ -x /usr/sbin/logcheck ]; then nice -n10 /usr/sbin/logcheck -R; fi 2 * * * * logcheck if [ -x /usr/sbin/logcheck ]; then nice -n10 /usr/sbin/logcheck; fi # EOF Change email destination Change SENDMAILTO variable to point to your email. File: /etc/logcheck/logcheck.conf ~ # Controls the address mail goes to: # *NOTE* the script does not set a default value for this variable! # Should be set to an offsite \"emailaddress@some.domain.tld\" #SENDMAILTO=\"logcheck\" SENDMAILTO=\"don@example.com\" ~ Sysstat - Gather System Usage Statistics The sysstat[1] package contains various utilities, common to many commercial Unixes, to monitor system performance and usage activity: iostat reports CPU statistics and input/output statistics for block devices and partitions. mpstat reports individual or combined processor related statistics. pidstat reports statistics for Linux tasks (processes) : I/O, CPU, memory, etc. tapestat reports statistics for tape drives connected to the system. cifsiostat reports CIFS statistics. Sysstat also contains tools you can schedule via cron or systemd to collect and historize performance and activity data: sar collects, reports and saves system activity information (see below a list of metrics collected by sar). sadc is the system activity data collector, used as a backend for sar. sa1 collects and stores binary data in the system activity daily data file. It is a front end to sadc designed to be run from cron or systemd. sa2 writes a summarized daily activity report. It is a front end to sar designed to be run from cron or systemd. sadf displays data collected by sar in multiple formats (CSV, XML, JSON, etc.) and can be used for data exchange with other programs. This command can also be used to draw graphs for the various activities collected by sar using SVG (Scalable Vector Graphics) format. Default sampling interval is 10 minutes but this can be changed of course (it can be as small as 1 second). Redhat Cockpit uses pmlogger.service [2] from systemd. Install from Cockpit\u0027s Overview, Metrics and history. RedHat pmstat [3] $ pmstat @ Mon Jun 12 10:02:01 2023 loadavg memory swap io system cpu 1 min swpd free buff cache pi po bi bo in cs us sy id 0.00 116224 231636 3284 13116m 0 0 0 17 348 387 0 0 100 0.00 116224 233328 3284 13116m 0 0 0 0 339 383 0 0 100 0.00 116224 228704 3284 13116m 0 0 0 0 333 358 0 0 100 0.00 116224 227192 3284 13116m 0 0 0 6 493 548 0 0 99 ^C $ pmstat -a /var/log/pcp/pmlogger/bob.example.com/20230610.0.xz -t 2hour -A 1hour -z Note: timezone set to local timezone of host \"bob.example.com\" from archive @ Sat Jun 10 01:00:00 2023 loadavg memory swap io system cpu 1 min swpd free buff cache pi po bi bo in cs us sy id 0.08 2048 7646m 6440 6591m 0 0 0 3 198 237 0 0 100 0.08 2048 7650m 6440 6596m 0 0 0 3 202 237 0 0 100 0.06 2048 7643m 6440 6600m 0 0 0 3 204 236 0 0 100 0.00 2048 7597m 6440 6624m 0 0 2 27 219 261 0 0 100 0.09 2048 7609m 6440 6629m 0 0 0 3 215 259 0 0 100 0.03 2048 7593m 6440 6633m 0 0 0 3 220 261 0 0 100 0.00 2048 7585m 6440 6638m 0 0 0 4 223 263 0 0 100 0.01 0 14402m 6740 495508 ? ? ? ? ? ? ? ? ? 0.00 0 14268m 6740 630344 ? ? ? ? ? ? ? ? ? 0.15 0 14272m 6740 634764 0 0 0 2 162 151 0 0 100 0.13 0 14266m 6740 639188 0 0 0 2 164 152 0 0 100 pmFetchGroup: End of PCP archive log Reference: 1 https://github.com/sysstat/sysstat 2 https://cockpit-project.org/guide/latest/feature-pcp.html 3 https://pcp.readthedocs.io/en/latest/UAG/MonitoringSystemPerformance.html#the-pmstat-command Installation The Debian Way $ sudo apt-get install sysstat The RedHat Way $ sudo dnf install sysstat Configuration It should configure itself, but just in case: The Debian Way $ sudo dpkg-reconfigure sysstat Replacing config file /etc/default/sysstat with new version The RedHat Way $ vi /etc/sysconfig/sysstat $ sudo systemctl enable --now sysstat The history files are kept here: The Debian Way $ ls /var/log/sysstat/ sa07 The RedHat Way $ ls /var/log/sa/ sa07 The timer is in the systemd configuration file. OnCalendar defines the interval. In this case data is collected every ten minutes. WantedBy defines that the timer should be active when the sysstat.service is running. Use systemctl edit sysstat-collect.timer to edit this file. It will automatically create an override file in the right place and enable it for you, and preserve the change over release updates. File: /usr/lib/systemd/system/sysstat-collect.timer # /lib/systemd/system/sysstat-collect.timer # (C) 2014 Tomasz Torcz \u003ctomek@pipebreaker.pl\u003e # # sysstat-12.5.2 systemd unit file: # Activates activity collector every 10 minutes [Unit] Description=Run system activity accounting tool every 10 minutes [Timer] OnCalendar=*:00/10 [Install] WantedBy=sysstat.service Above, Systemd edit override example changing the interval from 10 minutes to 5: # ls -lrt /etc/systemd/system/sysstat-collect.timer.d/ total 4 -rw-r--r--. 1 root root 27 Feb 19 09:38 override.conf # more /etc/systemd/system/sysstat-collect.timer.d/override.conf [Timer] OnCalendar=*:00/05 Reference: https://www.catalyst2.com/knowledgebase/server-management/how-to-install-configure-sysstat/ Past Statistics Report Report on system statistics over the last few days. File: sar.sh #!/bin/bash ################################# # Files are here: # ls -l /var/log/sysstat/ # -rw-r--r-- 1 root root 49064 Feb 9 16:35 sa09 # # Report on some other day: # sar -u 2 3 -f /var/log/sysstat/sa15 # # Output to file: # sar -u 2 3 -o /tmp/logfile ################################# echo \"Disk\" sar -d 2 3 echo \"Network\" sar -n DEV 2 3 echo \"CPU\" sar -u 2 3 sar -P ALL -u 2 3 echo \"Memory\" sar -r 2 3 echo \"Paging\" sar -B 2 3 echo \"Swap\" sar -S 2 3 echo \"Load\" sar -q 2 3 Sample Runs Memory $ sar -r Linux 5.10.120-ti-arm64-r64 (app.example.com) 11/07/2022 _aarch64_ (2 CPU) 02:09:36 PM kbmemfree kbavail kbmemused %memused kbbuffers kbcached kbcommit %commit kbactive kbinact kbdirty 02:10:01 PM 872556 2467364 1037140 27.37 162024 1529680 3898804 66.24 999112 1645384 536 Average: 872556 2467364 1037140 27.37 162024 1529680 3898804 66.24 999112 1645384 536 IO $ sar -b Linux 5.10.120-ti-arm64-r64 (app.example.com) 11/07/2022 _aarch64_ (2 CPU) 02:09:36 PM tps rtps wtps dtps bread/s bwrtn/s bdscd/s 02:10:01 PM 6.63 0.16 6.47 0.00 2.25 312.46 0.00 Average: 6.63 0.16 6.47 0.00 2.25 312.46 0.00 Network $ sar -n DEV Linux 5.10.120-ti-arm64-r64 (app.example.com) 11/07/2022 _aarch64_ (2 CPU) 02:09:36 PM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil 02:10:01 PM lo 3.94 3.94 1.66 1.66 0.00 0.00 0.00 0.00 02:10:01 PM eth0 4.10 5.27 0.37 1.72 0.00 0.00 0.00 0.00 02:10:01 PM usb0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 02:10:01 PM usb1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 02:10:01 PM docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Average: lo 3.94 3.94 1.66 1.66 0.00 0.00 0.00 0.00 Average: eth0 4.10 5.27 0.37 1.72 0.00 0.00 0.00 0.00 Average: usb0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Average: usb1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Average: docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Load and Run Queue $ sar -q Linux 5.10.120-ti-arm64-r64 (app.example.com) 11/07/2022 _aarch64_ (2 CPU) 02:09:36 PM runq-sz plist-sz ldavg-1 ldavg-5 ldavg-15 blocked 02:10:01 PM 3 531 0.74 0.61 0.51 0 Average: 3 531 0.74 0.61 0.51 0 S.M.A.R.T. Disk Monitoring Monitor and notify disk health using smartmontools, and email any notifications. Install software: The Debian Way $ sudo apt-get install smartmontools The RedHat Way $ sudo dnf install smartmontools Configure Add Long monitoring test for Sunday (/dev/sda through /dev/sdX) and comment out DEVICESCAN: The Debian Way File: /etc/smartd.conf The RedHat Way File: /etc/smartmontools/smartd.conf ~ # Don - 5-Nov-2021 # -a Default: equivalent to -H -f -t -l error -l selftest -C 197 -U 198 # -d TYPE Set the device type: ata, scsi, marvell, removable, 3ware,N, hpt,L/M/N # -n MODE No check. MODE is one of: never, sleep, standby, idle # -s REGE Start self-test when type/date matches regular expression (see man page) # T/MM/DD/d/HH # ^ ^ ^ ^ ^ # | | | | + 24 Hour # | | | +-- Day of week, 1(monday) through 7(sunday) # | | +----- Day of month, 1 ~ 31 # | +-------- Month of year, 01 (January) to 12 (December) # +---------- T is the type of test that should be run, options are: # # L for long self-test # S for short self-test # C for conveyance test # O for an Offline immediate Test # # -W D,I,C Monitor Temperature D)ifference, I)nformal limit, C)ritical limit # -m ADD Send warning email to ADD for -H, -l error, -l selftest, and -f #/dev/nvme0 -a -n never -W 2,30,40 -m don@example.com # Start long tests on Sunday 9am and short # self-tests every night at 2am and send errors to me #/dev/sda -a -n never -s (L/../../7/09|S/../.././02) -W 2,30,40 -m don@example.com -M test /dev/sda -a -n never -s (L/../../7/09|S/../.././02) -W 2,42,50 -m don@example.com -M diminishing #/dev/sdb -a -n never -s (L/../../7/09|S/../.././02) -W 2,30,40 -m don@example.com # Don - 5-Nov-2021 ~ #DEVICESCAN -d removable -n standby -m root -M exec /usr/share/smartmontools/smartd-runner ~ Restart Restart smartd daemon to pick up configuration changes $ sudo systemctl restart smartd Monitoring script for testing and reporting. Change the DEV below and see if your disks support SMART monitoring. #!/bin/bash DEV=\"/dev/sda\" # Info sudo smartctl -i \"${DEV}\" # Show sudo smartctl -P show \"${DEV}\" # turn smart on/off #sudo smartctl -s on \"${DEV}\" # Errors? sudo smartctl -l error \"${DEV}\" # Health Check sudo smartctl -Hc \"${DEV}\" # Selftest Log sudo smartctl -l selftest \"${DEV}\" # Attributes # Problems if... # Reallocated_Sector_Ct \u003e 0 # Current_Pending_Sector \u003e 0 sudo smartctl -A \"${DEV}\" # #.... T E S T S .... # -\u003e short ... couple of minutes # sudo smartctl -t short /dev/sda # -\u003e long ... one hour # sudo smartctl -t long /dev/sda # -\u003e Look at test results # sudo smartctl -a /dev/sda # #.... R E P O R T .... sudo smartctl --attributes --log=selftest \"${DEV}\" # # - Get the temprature sudo hddtemp /dev/sda smartd database The history of each smartd monitored disk is located here: $ ls /var/lib/smartmontools/ drivedb smartd.Samsung_SSD_980_1TB-S64ANS0T408956M.nvme.state~ smartd.Samsung_SSD_980_1TB-S64ANS0T418940T.nvme.state~ smartd.Samsung_SSD_980_1TB-S64ANS0T408956M.nvme.state smartd.Samsung_SSD_980_1TB-S64ANS0T418940T.nvme.state If you replace the disks and get error reports, you can remove the files and new ones will be created $ sudo rm /var/lib/smartmontools/smartd.Samsung_SSD_980_1TB-S64ANS0T4* Run smartctl for each disk: sudo smartctl -i /dev/sda sudo smartctl -i /dev/sdb ... Then check the history data files. New ones should show up. $ ls /var/lib/smartmontools/ attrlog.CT1000BX500SSD1-2251E695AE97.ata.csv smartd.CT1000BX500SSD1-2251E695AE97.ata.state smartd.CT1000BX500SSD1-2251E695AE9E.ata.state~ attrlog.CT1000BX500SSD1-2251E695AE9E.ata.csv smartd.CT1000BX500SSD1-2251E695AE97.ata.state~ smartd.Samsung_SSD_980_1TB-S64ANS0T408956M.nvme.state drivedb smartd.CT1000BX500SSD1-2251E695AE9E.ata.state smartd.Samsung_SSD_980_1TB-S64ANS0T418940T.nvme.state Login Notices to Users - motd/issue/issue.net You can customize the Message of the Day (motd), and infomation displayed when loggin in to the system. Installation The Debian Way $ sudo apt-get install cowsay fortune The RedHat Way $ sudo dnf install cowsay fortune-mod Configuration Update the message (/etc/motd) every hour. File: /etc/cron.hourly/motd The Debian Way #!/bin/bash /usr/games/cowsay $(/usr/games/fortune) \u003e /etc/motd The RedHat Way #!/bin/bash /bin/cowsay $(/bin/fortune) \u003e /etc/motd Verify File: /etc/motd _________________________________________ / Your mind is the part of you that says, \\ | \"Why\u0027n\u0027tcha eat that piece of cake?\" | | ... and then, twenty minutes later, | | says, \"Y\u0027know, if I were you, I | | wouldn\u0027t have done that!\" -- Steven and | \\ Ondrea Levine / ----------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || File: /etc/issue Look out! File: /etc/issue.net Looke out! Example login % ssh don@example Looke out! _________________________________________ / Individuality My words are easy to \\ | understand And my actions are easy to | | perform Yet no other can understand or | | perform them. My words have meaning; my | | actions have reason; Yet these cannot | | be known and I cannot be known. We are | | each unique, and therefore valuable; | | Though the sage wears coarse clothes, | | his heart is jade. -- Lao Tse, \"Tao Te | \\ Ching\" / ----------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || Last login: Sun Aug 21 10:13:57 2022 from 192.168.1.4 Login notification Add the following lines to the end of the system bashrc for notification whenever any user logs into the system (with the bash shell). The Debian Way File: /etc/bash.bashrc The RedHat Way File: /etc/bashrc ~ # Email logins - Don November 2020 echo $(who am i) \u0027 just logged on \u0027 $(hostname) \u0027 \u0027 $(date) $(who) | mail -s \"Login on\" don@example.com Continue Now that I have set up my new server, I\u0027ll consider giving it an internet name with DNS. Proceeding in the order presented, some things are depending on prior setups. Linux-in-the-House.svg"
     }
,     {
     "title": "NAS",
     "url": "https://dfcsoftware.github.io/nas.html",
     "body": "Network File Systems in a secure network are my home away from home. I used to have files scattered all over until i mounted NFS on every host. As usual, here are my notes: https://linux-in-the-house.pages.dev/NAS -Don #linux #opensource #nfs"
     }
,     {
     "title": "Encrypt Web",
     "url": "https://dfcsoftware.github.io/encrypt-web.html",
     "body": "If it runs over the Internet, Encrypt it. That\u0027s cowboy logic. If it hurts, hide it, if it\u0027s a load, truck it, if it\u0027s a punch, duck it... (Michael Martin Murphey) Close those curtains on your windows over there! E-Mail, Chat, Cloud, Router, etc, on 300 million websites using Lets Encrypt can\u0027t be wrong. https://linux-in-the-house.pages.dev/Certificate #letsencrypt #certbot #linux #opensource"
     }
,     {
     "title": "Streaming Music",
     "url": "https://dfcsoftware.github.io/streaming-music.html",
     "body": "In the spirit of fosstodon, I would like to submit my vote for \"Best music streaming radio station of 2022\". If you like music from the 1960\u0027s to 1970\u0027s it\u0027s a winner. If not, what streaming music radio do you like? #music #streaming #radio https://www.handcraftedradio.org/ I donate every three months, depending on how much I listened. Long live Free and Open Source Software, hardware, music and radio."
     }
,     {
     "title": "Book",
     "url": "https://dfcsoftware.github.io/book.html",
     "body": "I started writing a book this year about #linux and it\u2019s been great so far. I put the first part on-line here:https://Linux-in-the-house.org"
     }
,     {
     "title": "Introduction",
     "url": "https://dfcsoftware.github.io/introduction.html",
     "body": "Linux in the House The amazing things you can do with computer software in your home, ya GNU? By: Don Cohoon Website: https://linux-in-the-house.org RSS: https://fosstodon.org/@LinuxintheHouse.rss Feedback: Mastodon mail@linux-in-the-house.org This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Tux Image By lewing@isc.tamu.edu Larry Ewing and The GIMP, Attribution, https://commons.wikimedia.org/w/index.php?curid=574842 Created with: pelican: https://getpelican.com/ tinysearch: https://github.com/tinysearch/tinysearch INKSCAPE: https://inkscape.org/ vi: https://en.wikipedia.org/wiki/Vi Linux-in-the-House.svg Preface Everyone with some computer skills should take control of your data and programs in your house, with a few minutes work every day to gain knowledge, security, and improve your life. This guide will walk through from beginner level to intermediate. You should be able to find something useful to you no matter what your current level of experience, even the most advanced. This book will explore mostly the software side (programs) of a home computer, while briefly looking at what hardware will support it. Advanced users might want to jump ahead at this point to The Most Import Feature of All Build Your Own PC at MicroCenter Reference: https://www.microcenter.com/site/content/howtochooseyourpcparts.aspx Table of Contents Preface Build Your Own PC at MicroCenter Not just pieces and parts How to decide Why Open Source Evaluating a Project Be Curious, Just Try Ok, Jump in the Water, it\u0027s Just Fine Distribution Install Get Familiar with the Command Line Edit Files Tilde - text editor from the 1990\u0027s \ud83e\udd14 Most Important Feature of All Security Tips: Go to =\u003e Setup a New Server Conventions used in this book Debian RedHat Not just pieces and parts It is possible to build a computer from parts, and just as satisfying to have someone else build it. A pre-built PC is like working with a house builder or selecting a new car from the factory. Discovering options available and only pay for what you need will make ownership of the hardware and software more satisfying. For example, I like small form factor systems that consume less power. Partly to save electrical costs and enable longer run-time on a battery backup (UPS) system. That\u0027s why I chose the Intel NUC [1] and BeagleBone [2] Small Board Computer (SBC) systems for light duty jobs like e-mail and cloud, and larger ATX [3] motherboards in a server case for Network Attached Storage (NAS) [4], housing up to five SATA disk drives. https://www.intel.com/content/www/us/en/products/details/nuc.htm https://beagleboard.org/bone https://en.wikipedia.org/wiki/ATX https://www.truenas.com/truenas-scale/ How to decide The first thing to think about is what do you want to do. What computing feature of my life would you like to learn about and have control over. Next consider having a backup plan. Like when bad weather hits, where would you go? Same thing with building your computer system(s). I generally have two pieces of hardware for each task, then document and backup the software. Backup systems can be hand-me-downs or replaced systems and parts to save money. Finally you have to choose how much time and money you want to invest. Usually time equals money. Some things you may not be comfortable doing just now, and change your mind later when you have more experience. Why Open Source After investing your time and money it is a shame to find out that company you bought something from has discontinued your product or gone out of business. This can also occur in Open Source environments if the developers retire, move on or just get tired of working on it. With Open Source you can obtain the source code and keep it going yourself or find someone you can. Using your skill and knowledge while building your system will provide confidence and know how. Plus there are many resources on the Internet to ask questions, and answer along the way. Running your services on your own system can provide better security as hackers like to go for bigger targets. Your data is not on someone else\u0027s disk (cloud), but your own that only you provide access to. System administrators have access to everything on the computer so be your own Sys Admin over your own data. Advanced experience affords you the ability to change what you do not like or fix a problem the original owner does not. You have the source code, you can make the change. Evaluating a Project Open Source projects will post lots of information about themselves that you can use to decide if it is worth your time to use it. Does it do what you want Does it have an active community When was the last release Do they care about security What language does it use How does it fit on your existing hardware First I ask myself, \"self\", \"What are you trying to do?\" My answers were: e-mail, cloud storage, private contacts and calendar, read the news, monitor the house when I\u0027m out, listen to music, watch a movie or TV, and save my notes where I can find them. Then search the Internet for Open Source software that seems to do that. Sites like: https://opensource.com/alternatives https://www.linuxalt.com/ https://www.linuxjournal.com/ https://www.gnu.org/software/software.html are good starters. Avoid sites with lots of advertising, binary downloads and non-secure web servers. Do not download packages unless the Linux package manager cannot find it. Just look to see if the things you want are available, what the name is, then do $ sudo apt search \u003cname\u003e to see if it is already packaged for you. This way you get secure software and it will be updated automatically. Linux packages are a sign of active development and the distributions https://packages.ubuntu.com https://www.debian.org/distrib/packages https://fedora.pkgs.org/ https://software.opensuse.org/explore should have origination support website links back to each project. Remember once you pick a Linux distribution, you need to stick with it\u0027s packages. Smaller products can be found on github.com [1] and gitlab.com [2] and are generally compatible with any distribution. https://github.com/search https://gitlab.com/explore/projects Be Curious, Just Try Take some time to read the use-case and security pages. Passwords should be encrypted, network access should be none or limited, test cases should be extensive and developer participant counts made available. If you open this to the internet, expect hackers to come at it, so error logging is a must. Find out what programming language(s) they use, because you should have a quick look at it to decide if it is readable and something you might tackle if needed. Otherwise be prepared to discard it if a big problem arises. When you get some time, try learning bash [1], Python [2], C [3], C++ [4], and Rust [5]. That will have you covered for most projects, and open up new doors of fun. Lastly think about what computer will run it, does it have enough disk, memory, cpu and backup methodologies. https://linuxconfig.org/bash-scripting-tutorial-for-beginners https://www.learnpython.org/ https://www.cprogramming.com/ https://www.codecademy.com/learn/learn-c-plus-plus https://google.github.io/comprehensive-rust/welcome.html Ok, Jump in the Water, it\u0027s Just Fine Now you obtain the hardware, load up an operating system, and start installing packages to do something useful. [ ] Buy or find hardware [ ] Pick and install a Linux distribution on it [ ] Install packages \ud83d\ude03 For me I chose Ubuntu because it\u0027s been around a long time, supports all my hardware, has good package support and I\u0027ve used it for years. BeagleBone SBC comes with Debian, which Ubuntu is built from, so I stick with it. My laptop is Linux Mint, which has a nice user interface. My hardware is three levels: Small: BeagleBone AI and AI-64 single board computers (SBC) [1]. Applications: News, Music Medium: Intel NUC [2] boxes with SSD and NVME storage, and about 16\\~32MB of memory. The included one Ethernet and graphics built onto the motherboard seem to work just fine for my usage. Also consider a laptop, Dell XPS-13 [3] is a good one, as well as System76 [4]. Applications: E-Mail, Cloud, Home Automation Large: Gigabyte ATX motherboard [5] with SATA disk drive connections for NAS. Seagate Ironwolf 2TB disk drives, and 64GB of memory. Applications: NAS Try to install applications on suitable boxes, and don\u0027t forget battery backup to save the life of your computers from dropouts, surges, and spikes in electrical power. APC has a nice 865w back-ups [6] with an extension battery pack [7] that will run several systems for several hours. With less power demand, the longer a battery backup will last. One nice thing about laptops; built-in battery backup. https://beagleboard.org/bone https://www.intel.com/content/www/us/en/products/details/nuc.htm https://www.dell.com/en-us/shop/dell-laptops/xps-13-laptop/spd/xps-13-9315-laptop https://system76.com/laptops https://www.microcenter.com/site/content/howtochooseyourpcparts.aspx https://www.apc.com/us/en/product/BR1500G/apc-backups-pro-1500va-865w-tower-120v-10x-nema-515r-outlets-avr-lcd-user-replaceable-battery/ https://www.apc.com/us/en/product/BR24BPG/apc-backups-pro-external-battery-pack-for-1500va-backups-pro-models-formerly-backups-rs-1500/ Distribution Install This is documented well, and may change slightly from release to release so I will not waste time or space here, just list the common websites. Common Linux Distributions: Ubuntu: ubuntu.com Debian: debian.com Fedora: fedora.org Suse: opensuse.org Short List: https://linuxconfig.org/linux-download Long List: https://en.wikipedia.org/wiki/List_of_Linux_distributions Download and installation for Ubuntu can be found here: https://ubuntu.com/tutorials/install-ubuntu-desktop Get Familiar with the Command Line Linux command line for beginners: https://ubuntu.com/tutorials/command-line-for-beginners#1-overview Bash Guide for Beginners: https://tldp.org/LDP/Bash-Beginners-Guide/html/index.html Advanced Bash-Scripting Guide: https://tldp.org/LDP/abs/html/index.html Command line handbook: https://linuxhandbook.com/a-to-z-linux-commands/ Use man \u003ccommand\u003e to read the man page at the command line for any Linux command. Example: \u003cspace\u003e or CTRL-F to go forward CTRL-B to go back /\u003cword\u003e to search the page for \u003cword\u003e q to quit $ man rsync rsync(1) User Commands rsync(1) NAME rsync - a fast, versatile, remote (and local) file-copying tool SYNOPSIS Local: rsync [OPTION...] SRC... [DEST] Access via remote shell: Pull: rsync [OPTION...] [USER@]HOST:SRC... [DEST] Push: rsync [OPTION...] SRC... [USER@]HOST:DEST Access via rsync daemon: Pull: rsync [OPTION...] [USER@]HOST::SRC... [DEST] rsync [OPTION...] rsync://[USER@]HOST[:PORT]/SRC... [DEST] Push: Manual page rsync(1) line 1 (press h for help or q to quit) Edit Files Learn how to edit files with vi here [1] (thanks to the University of Tennessee) https://www.jics.utk.edu/files/images/csure-reu/PDF-DOC/VI-TUTORIAL.pdf Tilde - text editor from the 1990\u0027s \ud83e\udd14 Or edit with a simple text editor, tilde [1]. Hint: to open the file menu use ESC+F, Edit menu is ESC+E. $ sudo apt-get install tilde $ tilde https://github.com/gphalkes/tilde Most Important Feature of All SECURITY of course! Do not skimp on security. If you think it is hard, try recovering from Identity Theft or Ransomware. Lock the windows and doors to your computer. Security Tips: Start with your router. Change the administrative password BEFORE connecting to the Internet. Make sure all ports are closed for incoming access. Read =\u003e RouterSecurity.org [1] \u003c= for excellent advice. Use a password manager. KeePassXC [2] is a multi platform, offline database. It works on Linux, Phone, Windows, and Mac. Sync your password database with your own service. Nextcloud is excellent and easy. Set your passwords. Do not keep the default password for any application. Use different passwords for EVERY app. Look at your log files and e-mail alerts. The next steps will have services like Logwatch and Fail2ban. Read the messages filtered out by them. Remember the old saying; An ounce of prevention is worth a pound of cure. https://routersecurity.org/ https://keepassxc.org/ Go to =\u003e Setup a New Server Proceed in the order presented, some things are depending on prior setups. Conventions used in this book Text in code block format, like this -\u003e code block, are meant to be executed on the Linux command line, or text to be copied into a configuration file. Commands like $ sudo ..., should not type in the dollar sign ($). Just start with sudo .... The dollar sign is a common Linux prompt. All files have their code blocks preceded by -\u003e File: \u003cname\u003e. Anything surrounded by angle brackets, like \u003cname\u003e, is meant to be substituted by your particular name without the angle brackets. Hyperlinks in the document will open in the current web browser page. To open in a new tab/window use right-click and select \u0027open in new tab\u0027 from the on screen menu. All host names are given as example.com. You are expected substitute your domain name. Also check the hostname, as www and mail hosts may not exist on your network. RedHat and Debian differ in some commands, usually system administration. To show these alternatives without cluttering up the page, horizontal lines will be shown, like this: Debian The Debian Way Hi from Debian! RedHat The RedHat Way Hi from RedHat! {{#include date.txt}}"
     }
,     {
     "title": "NBS",
     "url": "https://dfcsoftware.github.io/nbs.html",
     "body": "The 20 year most useful award goes to -\u003e \"Never Before Seen\" (NBS) here:http://ranum.com/security/computer_security/code/index.html with a spinoff \"retail\" here:https://github.com/mbucc/retail Thanks Marcus Ranum \ud83c\udfc6 for making System Admin better. May your bowls always be round and spears straight. #linux #sysadmin"
     }
,     {
     "title": "JohnMastodon",
     "url": "https://dfcsoftware.github.io/johnmastodon.html",
     "body": "\u201cIn fact, he planted nurseries rather than orchards, built fences around them to protect them from livestock, left the nurseries in the care of a neighbor who sold trees on shares, and returned every year or two to tend the nursery.\u201d John Mastodon #JohnMastodon or Johnny Appleseed."
     }
,     {
     "title": "My Second Review",
     "url": "https://dfcsoftware.github.io/my-second-review.html",
     "body": "Following is a review of my favorite mechanical mouse."
     }
,     {
     "title": "My Writing Post",
     "url": "https://dfcsoftware.github.io/my-writing-post.html",
     "body": "Following is a story about my favorite ghost story."
     }
,     {
     "title": "Forest",
     "url": "https://dfcsoftware.github.io/pages/forest.html",
     "body": "2023 - October Forest Administration System Last Updated: 29-October-2023 Version: 2.x Finally got my alerts working, so when a host goes haywire I get an alert on the Phone and Cloud, with E-Mail as backup. Hope this helps, -- Don alert.sh and log.sh are meant to run a few times per hour and send alerts if watch thresholds are exceeded. alert.sh - Usually runs on one host and monitors other hosts, using a normal user ssh tunnel. log.sh - Usually runs on each host as root. When any new file is uploaded into the new alert user space, an entry will be written to the NextCloud Conversation with the name and link to that file. On your Phone you can install NextCloud Talk, log in as the new alert user and recieve notifications. Bonus: Your watch will alert you too! On your Phone you can install NextCloud Sync, log in as the new alert user and read notification files. Quick Start On Controller host If you do not have a controller host, just untar the files in ~/forest, otherwise deploy: a. ~/forest/bin/deploy.sh first-time \u003chost\u003e On Each Host Install (Dependencies: make gcc) a. cd ~/forest/install/db 1) unzip oracle_berkely_DB...zip 2) cd db-18.1.40/build_unix 3) ../dist/configure 4) make 5) sudo make install 6) rm -r db-18.1.40/ when done to save space b. cd ~/forest/install/ 1) tar -xvf nbs.tar 2) cd nbs 3) make clean 4) make c. cd ~/forest/install/ 1) tar -xvf retail.tar 2) cd retail 3) make clean 4) make d. cd ~/forest/install/ 1) tar -xvf cron.tar 2) cd cron 3) make clean 4) make e. cd ~/forest 1) ln -s /home/bob/forest/config/ /home/bob/.config/forest/ # ^ install location ^ user home/.config/ 2) sudo mkdir /root/.config sudo ln -s /home/bob/forest/config/forest.txt /root/.config/forest.txt # ^ install location ^ root home/.config/ f. cd ~/forest/install/ 1) install.sh 2) backup_links.sh On Controller host Configure and Deploy Configuration a. Create ~/forest/config/config.\u003chost\u003e parameters using examples in config/samples b. Create alert function config parameters (df.\u003chost\u003e, logwatch.\u003chost\u003e, util.\u003chost\u003e, etc) in ~/forest/config/ using examples in config/samples c. Create ~/forest/config/hosts.\u003chost\u003e file using example in config/samples d. Create ~/forest/config/cron.\u003chost\u003e schedule using example in config/samples f. Deploy configuration files to \u003chost\u003e 1) ~/forest/bin/deploy.sh host \u003chost\u003e 2) On the **target** \u003chost\u003e : Schedule the jobs: ~/forest/bin/schedule.sh All configuration changes need to be done on the controller host, then deploy.sh to \u003chost\u003e, or else the next deploy will overwrite them. Check Alerts, Jobs, and Watch Real-time stats a. ~/forest/bin/watch.sh - Watch alert functions run b. ~/forest/bin/react.sh - See alerts and log entries, and react on them c. ~/forest/bin/jobs.sh - Check scheduled cronjobs 1) backup_home.sh 2) backup_db.sh 3) cert_expire.sh 4) more... Schedule will wait for the duration of the specified forward date and time, at the time it is run. If daylight savings runs in-between the schedule execution time for this job and run time for this job , it does not change the wait time. For instance, in the fall, we fall back one hour, so instead of running at 8am the job will run at 7am; however the next runs will be at 8am as specified. Concepts The Forest implements ssh \u0027doors\u0027 to other hosts of the forest using alarms.sh for automated watching from the controller host. Conceptually, during a door invocation the client thread that issues the door procedure call migrates to the server process associated with the door, and starts executing the procedure while in the address space of the server. When the service procedure is finished, a door return operation is performed and the thread migrates back to the client\u0027s address space with the results, if any, from the procedure call. Installation Download https://github.com/dfcsoftware/watch Copy all files to ~/forest, or whatever directory you like, just change this documents\u0027 references of /home/bob to _your_ directory. Delete any lines in files /etc/issue and /etc/issue.net as they will interfere with function monitors doing ssh into a host causing alerts every time. Files: /etc/issue, /etc/issue.net $ sudo -i $ \u003e /etc/issue $ \u003e /etc/issue.net Configuration deploy.sh will create a config directory structure: $ mkdir -p ~/.config/forest Create config file: File: ~/.config/forest/config. export CLOUD_USER=\u003cnextcloud user\u003e export CLOUD_PASS=\"\u003cnextcloud password\u003e export CLOUD_DIR=alert export CLOUD_SERVER=\"https://www.example.com/nextcloud\" export CLOUD_LOG=/home/bob/forest/log/cloud.log export LOCAL_DIR=/home/bob/forest export SSH_USER=bob export LD_LIBRARY_PATH=/usr/local/BerkeleyDB.18.1/lib:$LD_LIBRARY_PATH Make sure the LOCAL_DIR/alert exists $ mkdir -p ${LOCAL_DIR}/alert Make sure CLOUD_LOG is writeable $ touch ${CLOUD_LOG} Create an hosts.txt list of hosts to monitor File: ~/.config/forest/hosts.txt # ssh Remote Remote Backup Function # Host Port Script Home Home Monitors # ---------------- ---- ------ ---------------------- ------ ---------------------------- vm1 22 0 /home/bob 1 df util memory 0 vm2 23 1 /home/sam 0 df 0 memory 0 # # Remote Script: 1=run moniter script that is on remote machine # 0=run monitor script on local, through ssh tunnel # Function Monitors: Forest script to run. 0 = skip. # Backup Home: 1=yes, 0=no Schedule alert.sh in cron i.e.: every 20 minutes File: /etc/cron.d/alert # Run the alert analysis SHELL=/bin/bash PATH=/sbin:/bin:/usr/sbin:/usr/bin MAILTO=\"bob@bob.com\" */20 * * * * root /home/bob/forest/alert.sh Copy ssh keys to remote If remote monitoring is desired; Generate and copy the linux ssh keys. $ ssh-key-gen $ ssh-copy-id \u003cremote hosts\u003e Make sure the directory leading up to the remote destination has 755 permissions, and the final home directory is 700. The remote desination must have the ~/.ssh directory, with chmod 700 Alert Functions This is a seperate file for each functional alert, and sometimes host. Remote Hosts need their own config file(s) On the Remote Host(s): $ mkdir -p ~/.config/forest Example of df functional monitor. Each script will describe it\u0027s own. File: ~/.config/forest/df. Usage-Percent-Limit File-System Mount-Point 34 \"/dev/mmcblk1p2\" \"/\" 38 \"/dev/mmcblk1p1\" \"/boot/firmware\" 16 \"/dev/md0\" \"/mnt/raid1\" Dependencies: Performance Co-Pilot - https://pcp.io/ dnf=RedHat; apt=Debian $ sudo dnf install pcp cockpit-pcp python3-pcp # default from cockpit web install $ sudo apt install pcp cockpit-pcp python3-pcp # default from cockpit web install # - Systemd Services: # pmcd.service # pmlogger.service # pmie.service # pmproxy.service # $ sudo dnf install pcp-export-pcp2json # pcp2json $ sudo apt install pcp-export-pcp2json # pcp2json # Debian 11 may need to add; # File: /etc/apt/sources.list.d/unstable.list # deb http://deb.debian.org/debian/ unstable main # Also may need to run: pip install requests $ sudo dnf install pcp-system-tools # pmrep # $ sudo dnf install jq # json parser $ sudo apt install jq # json parser # $ sudo dnf install jc # json commands $ sudo apt install jc # json commands # $ sudo dnf install ncdu # Text-based disk usage viewer $ sudo apt install ncdu # Text-based disk usage viewer # $ sudo dnf install nmon # Text-based system utilization viewer $ sudo apt install nmon # Text-based system utilization viewer Default is to send an e-mail if limits are exceeded To stop E-Mail, add a SEND_MAIL export to the config.txt file. 0 = NO 1 = YES File: ~/.config/forest/config.$HOST ~ export SEND_MAIL=0 ~ NextCloud Flow Notifications Create new alert user in NextCloud Add the NextCloud server as SERVER in ~/.config/forest/config.txt Add the NextCloud user as CLOUD_USER in ~/.config/forest/config.txt Add the NextCloud user\u0027s password as CLOUD_PASS in ~/.config/forest/config.txt As the new alert user in NextCloud; Go to Talk Create a new group Conversation Go to Files and create a new alert directory Add it as the CLOUD_DIR to ~/.config/forest/config.txt Go to Personal Settings \u003e Flow Add a new flow Write to conversasion (blue) When: File created and: File size (upload) is greater than 0 MB -\u003e Write to conversasion using the Conversation created above Reference: https://github.com/nextcloud/flow_notifications Example config.txt savelog.sh This is used to save off several copies of the last sync.sh process sending alerts to the NextCloud server. The logs are better viewed using the lnav Linu package. lnav ~/forest/log/ This is released on many OS packages, but not all, so it is included here. Thanks very much to the original authors! Reference: https://launchpad.net/ubuntu/xenial/+package/debianutils https://manpages.ubuntu.com/manpages/xenial/en/man8/savelog.8.html https://opensource.apple.com/source/uucp/uucp-10/uucp/contrib/savelog.sh.auto.html https://rhel.pkgs.org/8/lux/uucp-1.07-65.el8.lux.x86_64.rpm.html log.sh - Log Watcher This script runs as root on each node to search for Never Before Seen (NBS) entries in a log file. It needs to be scheduled in cron. The flow is: cron runs log.sh File: /etc/cron.d/logwatcher # Log - Watcher PATH=/usr/lib/sysstat:/usr/sbin:/usr/sbin:/usr/bin:/sbin:/bin MAILTO=\"bob@bob.com\" # Run Log watcher */20 * * * * bob /home/bob/forest/log.sh log.sh reads config file ~/.config/forest/logwatch. Up to two filters may be specified. File: logwatch.vm7 # File: logwatch.vm7 # DB File Alert Email Filter # ------------------------- ----------------------------------------- ------ ------ -----------------------------------# apache-access /var/log/apache2/access.log N Y geturl.pl skip_local_ips.sh apache-error /var/log/apache2/error.log Y Y geturl.pl apache-other_vhosts_access /var/log/apache2/other_vhosts_access.log Y Y geturl.pl New lines in File are read by retail, filtered by any and all Filters supplied. DB is checked if this has been seen before and can be ignored. Any remainging lines are sent to the alert directory and an Email sent, if Y in config.txt. A sync for new alert files is done, sending new files to the cloud (NextCloud). NextCloud will send a Talk alert to the CLOUD_USER for new files. These are viewable on a Phone or Watch, if running the Talk app. The following software packages have to be installed and compiled locally: * Berkerly DB - https://www.oracle.com/database/technologies/related/berkeleydb-downloads.html# Re-directs to oracle: https://www.oracle.com/database/technologies/related/berkeleydb-downloads.html# Need to log into oracle, download (via-download manager or wget.sh script proveded) $ cd ~/forest/install/db $ unzip \u003cdownload\u003e $ cd \u003cversion\u003e, i.e.: db-18.1.40 To perform a standard UNIX build of Berkeley DB; Change to the build_unix directory Enter the following two commands: $ ../dist/configure $ make This will build the Berkeley DB library. To install the Berkeley DB library, enter the following command: $ sudo make install Add/Create File: ${CONFIG_DIR}/config.${HOSTNAME} export LD_LIBRARY_PATH=/usr/local/BerkeleyDB.18.1/lib Never Before Seen (NBS) - http://ranum.com/security/computer_security/code/nbs.tar $ cd ~/forest/install $ tar -xvf install/nbs.tar $ cd nbs $ make clean $ make all Retail - https://github.com/mbucc/retail $ cd ~/forest/install $ tar -xvf install/retail.tar $ cd retail $ make clean $ make Install nbs and retail: $ cd ~/forest/install/ $ install.sh Refer to the log.sh script for more instructions. Reference: Marcus Ranum http://ranum.com/security/computer_security/code/index.html Logwatcher.sh Trouble Resolution armv7l Debian 10 The pmlogger systemd daemon had issues being installed, and the following log instructions solved it. Aug 08 11:07:39 bob.example.com systemd[1]: Starting LSB: Control pmlogger (the performance metrics logger for PCP)... Aug 08 11:07:43 bob.example.com pmlogger[913]: /etc/init.d/pmlogger: Warning: Performance Co-Pilot archive logger(s) not permanently enabled. Aug 08 11:07:43 bob.example.com pmlogger[913]: To enable pmlogger, run the following as root: Aug 08 11:07:44 bob.example.com pmlogger[913]: update-rc.d -f pmlogger remove Aug 08 11:07:44 bob.example.com pmlogger[913]: update-rc.d pmlogger defaults 94 06 Files . |-- Readme.md |-- Readme.sh |-+ alert * Monitered events |-+ bin * Executables |-+ config * Configurations |-+ filters * Log file selection criteria |-+ include * Compiler includes |-+ install * Installation packages and scripts |-+ lib * Compiled shared libraries |-+ log * Log files from Forest activities |-+ share * Compiler documentation |-+ status * Log watcher databases |-+ test * Unit test scripts |-+ tmp * Temporary files `-- version.txt git clone Clones a repository into a newly created directory, creates remote-tracking branches for each branch in the cloned repository (visible using git branch --remotes), and creates and checks out an initial branch that is forked from the cloned repository\u2019s currently active branch. After the clone, a plain git fetch without arguments will update all the remote-tracking branches, and a git pull without arguments will in addition merge the remote master branch into the current master branch, if any (this is untrue when \"--single-branch\" is given; see below). master changed to main in 2020 origin\u201d is not special Just like the branch name \u201cmaster\u201d does not have any special meaning in Git, neither does \u201corigin\u201d. While \u201cmaster\u201d is the default name for a starting branch when you run git init which is the only reason it\u2019s widely used, \u201corigin\u201d is the default name for a remote when you run git clone. If you run git clone -o booyah instead, then you will have booyah/master as your default remote branch. Remote-tracking branch names take the form /. For instance, if you wanted to see what the master branch on your origin remote looked like as of the last time you communicated with it, you would check the origin/master branch. If you were working on an issue with a partner and they pushed up an iss53 branch, you might have your own local iss53 branch, but the branch on the server would be represented by the remote-tracking branch origin/iss53. Downstream vs. Upstream |--------------------------- time -----------------------------\u003e |--- master ---- downstream -------\u003e ------- upstream ---------\u003e | | ^ | +--\u003e remote _clone_ from master _pull_ --\u003e + | [maybe need to] from remote _push_ --\u003e + | [_fetch/merge_] | [other people\u0027s] | [commits] Git remote repository access Create a bare repo, no working files There are two types of repos, one is \u0027bare\u0027 in that it contains no files that you can work with, only git indexed objects. It is suitable for sharing between other \u0027working files\u0027 repos. Create a sharing git repository on a system with raid disks and network access. $ mkdir backups/forest.git $ cd backups/forest.git $ git init --bare On working hosts where you want to track files, create a working repo, populate it with files, ane push it to the sharing repo. ```: $ mkdir ~/forest $ cd ~/forest $ git init $ [copy your files into the new directory] Then add them to your local git repo $ git add . $ git commit Push them to the sharing repo, with the --set-upstream option $ git push --set-upstream git+ssh://nas.doncohoon.com:/mnt/vol002/nfs2_share/backups/forest.git master Enumerating objects: 12507, done. Counting objects: 100% (12507/12507), done. Delta compression using up to 2 threads Compressing objects: 100% (6304/6304), done. Writing objects: 100% (12507/12507), 95.18 MiB | 12.71 MiB/s, done. Total 12507 (delta 6643), reused 9514 (delta 5966), pack-reused 0 remote: Resolving deltas: 100% (6643/6643), done. To git+ssh://nas.doncohoon.com:/mnt/vol002/nfs2_share/backups/forest.git * [new branch] master -\u003e master Branch \u0027master\u0027 set up to track remote branch \u0027master\u0027 from \u0027git+ssh://nas.doncohoon.com:/mnt/vol002/nfs2_share/backups/forest.git\u2019. You can see the sharing repo definition here: File: .git/config [branch \"master\"] remote = git+ssh://nas.doncohoon.com:/mnt/vol002/nfs2_share/backups/forest.git merge = refs/heads/master * Pushing commits to a remote repository Run ```git push origin main``` to push your local changes to your online repository. (push \u003csource\u003e \u003cdestination\u003e) * Pulling commits from a remote repository Run ```git pull origin``` to pull other committed changes to your local repository. (pull \u003csource\u003e) * Getting everything from a remote repository For new hosts you can grab all the files by using ```git clone *remotename*```. This will create a \u0027working directory\u0027 on your local machine. * Summary $ git remote add progit https://github.com/progit/progit2.git # [1] $ git fetch progit # [2] $ git branch --set-upstream-to=progit/master master # [3] $ git config --local remote.pushDefault origin # [4] 1. Add the source repository and give it a name. Here, I have chosen to call it progit. 2. Get a reference on progit\u2019s branches, in particular master. 3. Set your master branch to fetch from the progit remote. 4. Define the default push repository to origin. Once this is done, the workflow becomes much simpler: $ git checkout master # [1] $ git pull # [2] $ git push # [3] 1. If you were on another branch, return to master. 2. Fetch changes from progit and merge changes into master. 3. Push your master branch to origin. Reference: * \u003chttps://git-scm.com/book/en/v2\u003e : pp190 - Keep your GitHub public repository up-to-date ## Multiple remote git repositories 1. John and Jessica both clone into their remotes $ git clone john@githost:simplegit.git ... $ git commit -am \u0027Remove invalid default value\u0027 [master 738ee87] Remove invalid default value ...... $ git clone jessica@githost:simplegit.git ... $ git commit -am \u0027Add reset task\u0027 [master fbff5bc] Add reset task 1 files changed, 1 insertions(+), 0 deletions(-) 2. Jessica makes changes, commits into her remote, the pushes to the server $ git commit -am \u0027Add reset task\u0027 [master fbff5bc] Add reset task $ git push origin master ... To jessica@githost:simplegit.git 1edee6b..fbff5bc master -\u003e master 3. John makes changes, commits into his remote, then tries to push them to the same server $ git commit -am \u0027Remove invalid default value\u0027 [master 738ee87] Remove invalid default value 1 files changed, 1 insertions(+), 1 deletions(-) ... $ git push origin master ... ! [rejected] master -\u003e master (non-fast forward) error: failed to push some refs to \u0027john@githost:simplegit.git John must first fetch Jessica\u2019s upstream changes and merge them into his local repository before he will be allowed to push. 4. As a first step, John fetches Jessica\u2019s work [fbff5] (this only fetches Jessica\u2019s upstream work, it does not yet merge it into John\u2019s work): $ git fetch origin ... From john@githost:simplegit + 049d078...fbff5bc master -\u003e origin/master John\u0027s Master commit | [738ee] -\u003e [1edee] -\u003e [4b078] ^ [fbff5] ------+ | Origin Master 5. Now John\u0027s work [738ee] can merge with Jessica\u2019s work [fbff5] that he fetched into his own local work: $ git merge origin/master Origin Master | John [72bbc] -\u003e [738ee] -\u003e [1edee] -\u003e [4b078] | ^ | Jessica | +-----\u003e [fbff5] ------+ | Origin Master 6. At this point, John might want to test this new code to make sure none of Jessica\u2019s work affects any of his and, as long as everything seems fine, he can finally push the new merged work up to the server: $ git push origin master ... To john@githost:simplegit.git fbff5bc..72bbc59 master -\u003e master ``` Reference: https://git-scm.com/book/en/v2 : pp131 - Private Small Team"
     }
]